Loaded module: cuda/11.7
/zhome/49/a/165170/miniconda3/envs/hpc_env/lib/python3.10/site-packages/pytorch_lightning/cli.py:365: LightningDeprecationWarning: LightningCLI's 'save_config_overwrite' init parameter is deprecated from v1.8 and will be removed in v2.0.0. Use `save_config_kwargs={'overwrite': ...}` instead.
  rank_zero_deprecation(
Global seed set to 1234
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name | Type   | Params
--------------------------------
0 | l1   | Linear | 100 K 
1 | l2   | Linear | 1.3 K 
--------------------------------
101 K     Trainable params
0         Non-trainable params
101 K     Total params
0.407     Total estimated model params size (MB)
/zhome/49/a/165170/miniconda3/envs/hpc_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/zhome/49/a/165170/miniconda3/envs/hpc_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
`Trainer.fit` stopped: `max_epochs=5` reached.
Restoring states from the checkpoint path at /zhome/49/a/165170/Desktop/master_thesis_dtu/lightning_logs/version_1/checkpoints/epoch=4-step=8595.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from checkpoint at /zhome/49/a/165170/Desktop/master_thesis_dtu/lightning_logs/version_1/checkpoints/epoch=4-step=8595.ckpt
/zhome/49/a/165170/miniconda3/envs/hpc_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
