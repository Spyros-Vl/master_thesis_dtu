{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model try\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import cv2\n",
    "import os\n",
    "import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pathlib\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "#for model\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator, RPNHead, RegionProposalNetwork\n",
    "from torchvision.models.detection.faster_rcnn import *\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from coco_eval import CocoEvaluator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#DERT model\n",
    "from transformers import DetrForObjectDetection\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8507886904489268\n"
     ]
    }
   ],
   "source": [
    "from transformers import DetrConfig, DetrForObjectDetection\n",
    "\n",
    "config = DetrConfig.from_pretrained('facebook/detr-resnet-50',revision=\"no_timm\",num_labels=2,id2label={0:\"text\",1:\"fracture\"},\n",
    "                                                             ignore_mismatched_sizes=True) \n",
    "                                                             \n",
    "detr_model = DetrForObjectDetection(config)\n",
    "best_model = torch.load(f'../models/DETR_Model.pt',map_location=torch.device('cpu'))\n",
    "detr_model.load_state_dict(best_model['model_state_dict'])\n",
    "print(best_model['best_loss'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spiro\\OneDrive\\Υπολογιστής\\Thesis\\Project\\master_thesis_dtu\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spiro\\anaconda3\\lib\\site-packages\\transformers\\models\\detr\\image_processing_detr.py:780: FutureWarning: The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "%cd ..\n",
    "from src.data.my_rpg_dataset import CocoDetection\n",
    "from src.data.my_rpg_dataset import collate_fn_COCO\n",
    "from transformers import DetrImageProcessor\n",
    "\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "test_dataset_detr = CocoDetection(path_folder=\"data\", processor=processor,status='test')\n",
    "test_dataloader_detr = DataLoader(test_dataset_detr, collate_fn=collate_fn_COCO, batch_size=1, shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = CocoEvaluator(coco_gt=test_dataset_detr.coco, iou_types=[\"bbox\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "def plot_results(pil_img, scores, labels, boxes,target_labels,target_boxes,confidence=0.5):\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "\n",
    "    plt.imshow(np.asarray(pil_img),cmap='gray')\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    id2label = id2label={0:\"text\",1:\"fracture\"}\n",
    "    for label, box,c  in zip(target_labels.tolist(), target_boxes.tolist(), colors):\n",
    "        x,y,w,h = tuple(box)\n",
    "        ax.add_patch(plt.Rectangle((x,y),w,h,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        text = f'{model.config.id2label[label]}'\n",
    "        ax.text(x, y, text, fontsize=12,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.3))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Target Image and Boxes\")\n",
    "    ########################################################################################################\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "\n",
    "    plt.imshow(np.asarray(pil_img),cmap='gray')\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    id2label = id2label={0:\"text\",1:\"fracture\"}\n",
    "    for score, label, (xmin, ymin, xmax, ymax),c  in zip(scores.tolist(), labels.tolist(), boxes.tolist(), colors):\n",
    "        if score >= confidence:\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                    fill=False, color=c, linewidth=3))\n",
    "            text = f'{model.config.id2label[label]}: {score:0.2f}'\n",
    "            ax.text(xmin, ymin, text, fontsize=12,\n",
    "                    bbox=dict(facecolor='yellow', alpha=0.3))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Model prediction\")\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "detr_model.eval()\n",
    "\n",
    "# Iterate over all batches in the test loader\n",
    "for idx, batch in enumerate(tqdm(test_dataloader_detr)):\n",
    "    # get the inputs\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    pixel_mask = batch[\"pixel_mask\"].to(device)\n",
    "    labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]] # these are in DETR format, resized + normalized\n",
    "\n",
    "    target = labels \n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "      outputs = detr_model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    # turn into a list of dictionaries (one item for each example in the batch)\n",
    "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes)\n",
    "\n",
    "    # load image based on ID\n",
    "    image_id = target['image_id'].item()\n",
    "    image = test_dataset_detr.coco.loadImgs(image_id)[0]\n",
    "    image = Image.open(os.path.join(image['file_name']))\n",
    "    target_annotations = test_dataset_detr.coco.imgToAnns[image_id]\n",
    "\n",
    "\n",
    "    # postprocess model outputs\n",
    "    width, height = image.size\n",
    "    postprocessed_outputs = processor.post_process_object_detection(outputs,\n",
    "                                                                    target_sizes=[(height, width)],\n",
    "                                                                    threshold=0.5)\n",
    "    results = postprocessed_outputs[0]\n",
    "    plot_results(image, results['scores'], results['labels'], results['boxes'],torch.tensor([target_annotations[0]['category_id']]),torch.tensor([target_annotations[0]['bbox']]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "0b00de58b91327812a3543b0a54b44486ebb363ab132784d28226d0402cd7527"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
