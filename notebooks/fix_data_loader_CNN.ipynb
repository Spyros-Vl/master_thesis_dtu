{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for data visualisation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "from cv2 import cv2\n",
    "import torch\n",
    "from torchvision.ops import box_convert\n",
    "from typing import List, Dict\n",
    "from skimage.io import imread\n",
    "import pickle\n",
    "\n",
    "#for model\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator, RPNHead, RegionProposalNetwork\n",
    "import torch\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from cv2 import cv2\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pathlib\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#for model\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator, RPNHead, RegionProposalNetwork\n",
    "import torch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a test train folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations: 20327\n",
      "Number of images: 20327\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import shutil\n",
    "from cv2 import cv2\n",
    "import json\n",
    "import pathlib\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "#Set path to root dir (wrist folder)\n",
    "root_dir = '../literature/Other/supervisely/'\n",
    "\n",
    "# Path to annotation dir\n",
    "ann_dir = os.listdir(root_dir + 'wrist/ann')\n",
    "\n",
    "# Path to image dir\n",
    "img_dir = os.listdir(root_dir + 'wrist/img')\n",
    "\n",
    "pickle_list = os.listdir(root_dir + 'wrist/pickle_data')\n",
    "\n",
    "\n",
    "\n",
    "print('Number of annotations: {}'.format(len(ann_dir)))\n",
    "print('Number of images: {}'.format(len(img_dir)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The working directory of this project is: c:\\Users\\spiro\\OneDrive\\Υπολογιστής\\Thesis\\Project\\master_thesis_dtu\\notebooks\n"
     ]
    }
   ],
   "source": [
    "#os.chdir('../')\n",
    "\n",
    "print(\"The working directory of this project is:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"../literature/Other/supervisely/wrist/testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations: 1000\n"
     ]
    }
   ],
   "source": [
    "#Set path to root dir (wrist folder)\n",
    "root_dir = pathlib.Path('../literature/Other/supervisely')\n",
    "\n",
    "# Path to annotation dir\n",
    "ann_dir = pathlib.Path(root_dir, 'wrist/rpg_ann')\n",
    "\n",
    "# Path to image dir\n",
    "img_dir = pathlib.Path(root_dir, 'wrist/rpg_images')\n",
    "\n",
    "# Lists\n",
    "list_files = lambda start_dir: [str(item) for item in start_dir.iterdir()]\n",
    "ann_list = list_files(ann_dir)\n",
    "img_list = list_files(img_dir)\n",
    "print('Number of annotations: {}'.format(len(ann_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def data_fix(image_list,ann_list):\n",
    "\n",
    "    root_dir = pathlib.Path('../literature/Other/supervisely')\n",
    "\n",
    "    for idx in tqdm.tqdm(range(2000)):#tqdm.tqdm(range(len(img_list))):\n",
    "        img_path = os.path.join(img_list[idx])\n",
    "        ann_path = os.path.join(ann_list[idx])\n",
    "        \n",
    "        #target = []\n",
    "        d = {}\n",
    "\n",
    "        box = []\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        area = []\n",
    "\n",
    "        classes = {'fracture' : 1, 'text' : 2}\n",
    "\n",
    "        \n",
    "\n",
    "        with open(ann_path) as json_file:\n",
    "\n",
    "            #Load the img\n",
    "            name = pathlib.PurePath(ann_path).name.split('.')[0]\n",
    "            image = img_dir / (str(name) + '.png')\n",
    "            #img = Image.open(img_path).convert(\"RGB\")\n",
    "            # Load the grayscale image file\n",
    "            img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            # Convert the grayscale image to RGB and duplicate the grayscale channel\n",
    "            img = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB)    \n",
    "\n",
    "            obj_ids = np.unique(img)\n",
    "            num_objs = len(obj_ids)\n",
    "            \n",
    "            #Load the JSON file\n",
    "            data = json.load(json_file)\n",
    "\n",
    "\n",
    "        for object_dict in data['objects']:\n",
    "    \n",
    "            # Check if object contains any fractures\n",
    "\n",
    "            if object_dict['classTitle'] == \"text\" or object_dict['classTitle'] == \"fracture\":\n",
    "\n",
    "                # Get points and convert them to int for display purposes\n",
    "                top_left_point, bottom_right_point = object_dict['points']['exterior']\n",
    "                top_left_point = list(map(int,top_left_point))\n",
    "                bottom_right_point = list(map(int, bottom_right_point))\n",
    "                box = (top_left_point+bottom_right_point)\n",
    "                area.append((box[3] - box[1]) * (box[2] - box[0]))\n",
    "                label = classes[object_dict['classTitle']]\n",
    "                labels.append(label)\n",
    "                boxes.append(box)\n",
    "                \n",
    "\n",
    "        boxes = torch.FloatTensor(boxes)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        area = torch.as_tensor(area, dtype=torch.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "                \n",
    "        \n",
    "        d[\"boxes\"] = boxes\n",
    "        d[\"labels\"] = labels\n",
    "        d[\"image_id\"] = image_id\n",
    "        d[\"area\"] = area\n",
    "        d[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        target = d\n",
    "        img = img_path\n",
    "\n",
    "        ready_data = {'image' : img,'target' : target}\n",
    "\n",
    "        path = os.path.join(root_dir,'wrist','testing', name + '.pickle')\n",
    "\n",
    "        with open(path, 'wb') as handle:\n",
    "            pickle.dump(ready_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1000/2000 [02:06<02:06,  7.89it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\spiro\\OneDrive\\Υπολογιστής\\Thesis\\Project\\master_thesis_dtu\\notebooks\\fix_data_loader_CNN.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/fix_data_loader_CNN.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data_fix(img_list,ann_list)\n",
      "\u001b[1;32mc:\\Users\\spiro\\OneDrive\\Υπολογιστής\\Thesis\\Project\\master_thesis_dtu\\notebooks\\fix_data_loader_CNN.ipynb Cell 9\u001b[0m in \u001b[0;36mdata_fix\u001b[1;34m(image_list, ann_list)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/fix_data_loader_CNN.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m root_dir \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPath(\u001b[39m'\u001b[39m\u001b[39m../literature/Other/supervisely\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/fix_data_loader_CNN.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(\u001b[39mrange\u001b[39m(\u001b[39m2000\u001b[39m)):\u001b[39m#tqdm.tqdm(range(len(img_list))):\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/fix_data_loader_CNN.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(img_list[idx])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/fix_data_loader_CNN.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     ann_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ann_list[idx])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/fix_data_loader_CNN.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m#target = []\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "data_fix(img_list,ann_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XRayDataSet(torch.utils.data.Dataset):\n",
    "    \n",
    "\n",
    "     def __init__(self, root):\n",
    "        \n",
    "        self.root = root\n",
    "        self.instances = list(sorted(os.listdir(root)))\n",
    "        \n",
    "\n",
    "    \n",
    "     def __len__(self):\n",
    "        return len(self.instances) \n",
    "\n",
    "     def __getitem__(self,idx):\n",
    "        \n",
    "        instance = os.path.join(self.root, self.instances[idx])\n",
    "        \n",
    "\n",
    "        with open(instance, 'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "\n",
    "        img_path = data['image']\n",
    "        img = cv2.imread(img_path)\n",
    "        img = T.ToTensor()(img).float() \n",
    "        \n",
    "        target = data['target']\n",
    "\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = list(sorted(os.listdir(pathlib.Path('../literature/Other/supervisely/wrist/testing/'))))\n",
    "instance = os.path.join(pathlib.Path('../literature/Other/supervisely/wrist/testing/'), instances[0])\n",
    "\n",
    "with open(instance, 'rb') as handle:\n",
    "        data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n",
    "img_path = data['image']\n",
    "img = cv2.imread(img_path)\n",
    "img = T.ToTensor()(img).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src.data.my_dataset import collate_fn\n",
    "from torchvision.models.detection.faster_rcnn import *\n",
    "\n",
    "\n",
    "dataset = XRayDataSet(pathlib.Path('../literature/Other/supervisely/wrist/testing/'))\n",
    "training_dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0,collate_fn=collate_fn)\n",
    "images , targets = next(iter(training_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "      \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n",
      "----------------------train started--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 92/500 [1:03:49<4:43:04, 41.63s/it]\n"
     ]
    }
   ],
   "source": [
    "#load the model\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "#defines\n",
    "NumOfClasses = 3 \n",
    "NumOfEpochs = 10\n",
    "BatchSize = 2\n",
    "num_workers = 0\n",
    "\n",
    "model = get_model_instance_segmentation(NumOfClasses)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1)\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "\n",
    "print('----------------------train started--------------------------')\n",
    "\n",
    "for epoch in range(10):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    i = 0    \n",
    "    epoch_loss = 0\n",
    "    for imgs, annotations in tqdm(training_dataloader):\n",
    "           #imgs, annotations = imgs.to(device), annotations.to(device)\n",
    "        i += 1\n",
    "        imgs =list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "\n",
    "\n",
    "        loss_dict = model(imgs, annotations) \n",
    "        losses = sum(loss for loss in loss_dict.values())        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        #lr_scheduler.step() \n",
    "        epoch_loss += losses\n",
    "        \n",
    "    train_loss.append(epoch_loss)\n",
    "\n",
    "        \n",
    "    print(f'Epoch {epoch+1}: train_loss={epoch_loss}, val_loss={0}, time : {time.time() - start}')\n",
    "\n",
    "\n",
    "    print('----------------------train ended--------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "0b00de58b91327812a3543b0a54b44486ebb363ab132784d28226d0402cd7527"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
