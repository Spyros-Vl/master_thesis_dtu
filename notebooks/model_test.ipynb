{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model try\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import cv2\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pathlib\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.data.my_dataset import XRayDataSet\n",
    "from src.data.my_dataset import collate_fn\n",
    "from tqdm import tqdm\n",
    "\n",
    "#for model\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator, RPNHead, RegionProposalNetwork\n",
    "from torchvision.models.detection.faster_rcnn import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spiro\\OneDrive\\Υπολογιστής\\Thesis\\Project\\master_thesis_dtu\\notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = XRayDataSet(pathlib.Path('../literature/Other/supervisely/wrist/train_pickles'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=4,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images , targets = next(iter(training_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "      \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model_instance_segmentation(3)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy boxes and images format\n",
    "images, boxes = torch.rand(1, 3, 600, 1200), torch.rand(1, 2, 4) \n",
    "labels = torch.randint(1, 91, (1, 2))\n",
    "images = list(image for image in images)\n",
    "targets = []\n",
    "for i in range(len(images)):\n",
    "    d = {}\n",
    "    d['boxes'] = boxes[i]\n",
    "    d['labels'] = labels[i]\n",
    "    targets.append(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real try of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only predictions with score higher than the threshold\n",
    "def make_prediction(model, img, threshold):\n",
    "    model.eval()\n",
    "    preds = model(img)\n",
    "    print(preds)\n",
    "    for id in range(len(preds)) :\n",
    "        idx_list = []\n",
    "\n",
    "        for idx, score in enumerate(preds[id]['scores']) :\n",
    "            if score > threshold : \n",
    "                idx_list.append(idx)\n",
    "\n",
    "        preds[id]['boxes'] = preds[id]['boxes'][idx_list]\n",
    "        preds[id]['labels'] = preds[id]['labels'][idx_list]\n",
    "        preds[id]['scores'] = preds[id]['scores'][idx_list]\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train data\n",
    "train_dataset = XRayDataSet(pathlib.Path('../literature/Other/supervisely/wrist/pickle_data'))\n",
    "training_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4,collate_fn=collate_fn)\n",
    "\n",
    "#load validation data\n",
    "validation_dataset = XRayDataSet(pathlib.Path('../literature/Other/supervisely/wrist/validation_pickles'))\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=2, shuffle=False, num_workers=4,collate_fn=collate_fn)\n",
    "\n",
    "#load test data\n",
    "test_dataset = XRayDataSet(pathlib.Path('../literature/Other/supervisely/wrist/test_pickles'))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=4,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_instance_segmentation(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1)\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------train started--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 17428, 2848, 19412, 6128) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1120\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1121\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\multiprocessing\\queues.py:108\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 108\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    109\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\spiro\\OneDrive\\Υπολογιστής\\Thesis\\Project\\master_thesis_dtu\\notebooks\\model_test.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m    \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m epoch_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m imgs, annotations \u001b[39min\u001b[39;00m tqdm(training_dataloader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     imgs \u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(img\u001b[39m.\u001b[39msqueeze(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m imgs)\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1281\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 1282\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1283\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1284\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1132\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1133\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[0;32m   1135\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 17428, 2848, 19412, 6128) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "print('----------------------train started--------------------------')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    i = 0    \n",
    "    epoch_loss = 0\n",
    "    for imgs, annotations in tqdm(training_dataloader):\n",
    "        i += 1\n",
    "        \n",
    "        imgs =list(img.squeeze(dim=0).to(device) for img in imgs)\n",
    "        annotations = [{k: v for k, v in t[0].items()} for t in annotations]\n",
    "\n",
    "        ####-----------MOVE annotations to device---------------#####\n",
    "\n",
    "        # Iterate over the list of dicts and move each tensor to the device\n",
    "        for annotation in annotations:\n",
    "            for key, value in annotation.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    annotation[key] = value.to(device)\n",
    "        \n",
    "        ####-----------MOVE annotations to device---------------#####\n",
    "\n",
    "\n",
    "        loss_dict = model(imgs, annotations) \n",
    "        losses = sum(loss for loss in loss_dict.values())        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "        epoch_loss += losses\n",
    "    print(f'epoch : {epoch+1}, Loss : {epoch_loss}, time : {time.time() - start}')\n",
    "\n",
    "print('----------------------train ended--------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model state\n",
    "torch.save(model.state_dict(),f'my_first_model.pt')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test data\n",
    "test_dataset = XRayDataSet(pathlib.Path('../literature/Other/supervisely/wrist/test_pickles'))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the model state\n",
    "\n",
    "model = get_model_instance_segmentation(3)\n",
    "model.load_state_dict(torch.load(f'my_first_model.pt',map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_image_from_output(img, annotation):\n",
    "    \n",
    "    img = img.cpu().permute(1,2,0)\n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(img,cmap='gray')\n",
    "    \n",
    "    for idx in range(len(annotation[\"boxes\"])):\n",
    "        xmin, ymin, xmax, ymax = annotation[\"boxes\"][idx].detach().numpy()\n",
    "\n",
    "        if annotation['labels'][idx] == 1 :\n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "        \n",
    "        elif annotation['labels'][idx] == 2 :\n",
    "            \n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')\n",
    "            \n",
    "        else :\n",
    "        \n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='orange',facecolor='none')\n",
    "\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[478.8079,  96.8675, 521.0992, 168.2464],\n",
      "        [471.1464,  75.4242, 515.7393, 148.4656]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[ 517.5782, 1032.7249,  548.2688, 1069.0245],\n",
      "        [  46.6126, 1034.7902,   76.9252, 1073.2703],\n",
      "        [ 514.4083, 1042.4022,  544.9802, 1078.7648]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0645, 0.0547, 0.0523])}]\n",
      "[{'boxes': tensor([[502.7681,  85.8491, 546.7512, 162.2919]]), 'labels': tensor([2]), 'scores': tensor([0.0501])}]\n",
      "[{'boxes': tensor([[ 38.0739, 608.3450,  63.5054, 640.4083],\n",
      "        [ 32.4669, 578.2682,  76.7204, 641.3101],\n",
      "        [537.0625, 593.7674, 563.3991, 625.2963],\n",
      "        [534.1592, 602.0011, 560.5455, 633.7413],\n",
      "        [ 39.5010, 598.5244,  65.6942, 631.0235]]), 'labels': tensor([2, 2, 2, 2, 2]), 'scores': tensor([0.0564, 0.0551, 0.0542, 0.0515, 0.0511])}]\n",
      "[{'boxes': tensor([[498.0646,  74.3941, 530.5534, 129.2135],\n",
      "        [492.1789,  57.9255, 526.4359, 114.0217]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[584.4451,  87.3012, 622.5680, 151.6312],\n",
      "        [577.5385,  67.9755, 617.7365, 133.8040]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[527.1060,  64.7467, 565.2210, 127.8756],\n",
      "        [533.9320,  83.4249, 569.6937, 144.4028]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0526, 0.0511])}]\n",
      "[{'boxes': tensor([[ 765.6682, 1184.1481,  803.2108, 1229.0194],\n",
      "        [  57.3997, 1209.1686,   93.5598, 1254.5184],\n",
      "        [  56.3587, 1190.8672,   93.6530, 1237.2822]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0535, 0.0518, 0.0505])}]\n",
      "[{'boxes': tensor([[493.0983,  89.2281, 531.7045, 154.7697],\n",
      "        [485.8579,  68.8890, 526.9655, 137.2344]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0571, 0.0561])}]\n",
      "[{'boxes': tensor([[492.5293,  81.6240, 531.7856, 148.3052]]), 'labels': tensor([2]), 'scores': tensor([0.0568])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[ 27.3993, 455.7142,  46.5329, 480.3279],\n",
      "        [417.1984,  62.3756, 444.4117, 108.3384],\n",
      "        [412.2682,  48.5675, 440.9626,  95.6011]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0650, 0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[  44.3046,  984.7358,   73.0700, 1021.2230],\n",
      "        [ 388.9404,  982.8073,  418.4267, 1017.3965]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0547, 0.0517])}]\n",
      "[{'boxes': tensor([[  77.1258,  997.9255,  125.9406, 1059.4468]]), 'labels': tensor([2]), 'scores': tensor([0.0536])}]\n",
      "[{'boxes': tensor([[ 58.0668, 828.8091,  99.7995, 881.9978],\n",
      "        [ 53.4940, 795.1852, 125.9384, 898.0305],\n",
      "        [ 65.5659, 814.2023, 107.0970, 868.5802],\n",
      "        [893.2085, 133.5293, 951.4718, 231.9233],\n",
      "        [882.6534, 103.9701, 944.0877, 204.6558]]), 'labels': tensor([2, 2, 2, 2, 2]), 'scores': tensor([0.0625, 0.0601, 0.0531, 0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[621.4892,  80.0203, 659.0191, 151.2553]]), 'labels': tensor([2]), 'scores': tensor([0.0526])}]\n",
      "[{'boxes': tensor([[740.6646, 110.6276, 788.9778, 192.1466],\n",
      "        [731.9121,  86.1378, 782.8547, 169.5562]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[ 48.6739, 755.2479,  79.9486, 794.6589],\n",
      "        [665.3115,  99.4390, 708.7092, 172.7127],\n",
      "        [657.4493,  77.4259, 703.2092, 152.4074]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0589, 0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[ 32.9205, 753.9587,  77.8130, 817.9698],\n",
      "        [544.3141, 769.7863, 571.0030, 801.6843],\n",
      "        [ 40.8059, 787.5731,  66.5116, 819.8115],\n",
      "        [ 40.0655, 774.5632,  66.5782, 807.5579]]), 'labels': tensor([2, 2, 2, 2]), 'scores': tensor([0.0547, 0.0535, 0.0518, 0.0505])}]\n",
      "[{'boxes': tensor([[518.5822, 852.5450, 544.4803, 883.2932],\n",
      "        [ 34.4376, 855.2458,  58.7522, 886.6256],\n",
      "        [524.4703, 857.8911, 550.1219, 888.2896]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0636, 0.0610, 0.0506])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[ 436.9568, 1042.1825,  468.2491, 1079.3424],\n",
      "        [  46.9683, 1044.5522,   77.4636, 1083.2555]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0609, 0.0547])}]\n",
      "[{'boxes': tensor([[524.3958, 900.7926, 551.0247, 932.8027],\n",
      "        [ 40.6520, 902.7653,  67.0886, 936.3358]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0583, 0.0547])}]\n",
      "[{'boxes': tensor([[481.7462, 706.5803, 505.5873, 734.9276],\n",
      "        [ 29.2635, 692.6923,  69.7410, 749.4653],\n",
      "        [ 34.1386, 713.9006,  57.2781, 743.2451]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0613, 0.0600, 0.0596])}]\n",
      "[{'boxes': tensor([[ 37.9384, 842.7839,  62.5710, 874.0107]]), 'labels': tensor([2]), 'scores': tensor([0.0547])}]\n",
      "[{'boxes': tensor([[533.9031,  79.7469, 568.7298, 138.5101],\n",
      "        [ 39.4777, 582.3428,  64.2319, 613.8007]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0501])}]\n",
      "[{'boxes': tensor([[480.5920,  62.1090, 511.3440, 113.7431]]), 'labels': tensor([2]), 'scores': tensor([0.0535])}]\n",
      "[{'boxes': tensor([[  42.5817, 1020.7323,   72.4452, 1059.0425],\n",
      "        [  47.8647, 1013.6194,   78.2520, 1051.9298],\n",
      "        [ 638.1068, 1020.5541,  669.4940, 1057.9352]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0653, 0.0534, 0.0528])}]\n",
      "[{'boxes': tensor([[ 513.1363, 1025.3833,  543.9117, 1061.8999],\n",
      "        [  46.2281, 1027.6541,   76.2910, 1065.8684]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0605, 0.0547])}]\n",
      "[{'boxes': tensor([[ 35.1736, 842.9814,  59.8130, 874.5681],\n",
      "        [ 39.5263, 837.1080,  64.6073, 868.7230],\n",
      "        [527.0584, 842.8150, 553.0375, 873.7041]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0666, 0.0542, 0.0534])}]\n",
      "[{'boxes': tensor([[ 41.3337, 918.8223,  68.2134, 952.9899],\n",
      "        [430.6373, 919.3683, 458.6924, 952.0948]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0547, 0.0536])}]\n",
      "[{'boxes': tensor([[492.5513,  73.5872, 524.6799, 127.8120],\n",
      "        [486.7307,  57.2974, 520.6082, 112.7851]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[608.3373,  91.0048, 648.0190, 158.0644],\n",
      "        [601.1486,  70.8591, 642.9900, 139.4812]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[  40.4577, 1004.0456,   69.0091, 1040.9003]]), 'labels': tensor([2]), 'scores': tensor([0.0593])}]\n",
      "[{'boxes': tensor([[ 38.7693, 928.3432,  65.8447, 963.2419],\n",
      "        [580.2505, 928.2156, 608.8313, 962.2232]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0639, 0.0566])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[ 43.2284, 959.8571,  71.3400, 995.5509]]), 'labels': tensor([2]), 'scores': tensor([0.0547])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[ 31.9213, 764.6539,  54.3083, 793.3529],\n",
      "        [ 35.8818, 759.3254,  58.6611, 788.0246],\n",
      "        [478.3548, 764.5207, 501.8833, 792.5232]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0653, 0.0534, 0.0528])}]\n",
      "[{'boxes': tensor([[ 36.4486, 810.6438,  60.1131, 840.6804],\n",
      "        [407.1862, 809.0566, 431.4440, 837.5305]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0547, 0.0517])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[396.6501,  85.3995, 433.8818, 148.3284],\n",
      "        [389.9049,  66.4946, 429.1633, 130.8897]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[451.3695,  70.5620, 482.1537, 122.5571],\n",
      "        [445.7926,  54.9418, 478.2523, 108.1480]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[526.1624,  74.7591, 561.2766, 141.3118]]), 'labels': tensor([2]), 'scores': tensor([0.0526])}]\n",
      "[{'boxes': tensor([[274.5705,  63.0165, 302.0273, 109.4108],\n",
      "        [269.5343,  48.9711, 298.5711,  96.6628]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0529, 0.0523])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[ 48.4791, 815.8812,  79.9544, 855.7949],\n",
      "        [665.3118,  99.4076, 708.7094, 172.6582],\n",
      "        [ 49.4444, 838.5513,  80.2604, 877.2148]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0532, 0.0524, 0.0503])}]\n",
      "[{'boxes': tensor([[572.4988,  85.5697, 609.8425, 148.6242],\n",
      "        [565.7334,  66.6274, 605.1098, 131.1505]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[ 382.6804, 1128.5817,  416.8815, 1168.8997],\n",
      "        [  50.9701, 1131.1327,   84.1169, 1173.1954]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0549, 0.0547])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[ 625.9493,  968.0934,  656.6659, 1004.7736],\n",
      "        [  47.0223,  991.9976,   76.4516, 1029.0375],\n",
      "        [  46.1432,  970.3986,   76.1767, 1008.6588]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0541, 0.0506, 0.0502])}]\n",
      "[{'boxes': tensor([[ 473.8817, 1001.4321,  503.6202, 1036.6743],\n",
      "        [  45.1178, 1003.4844,   74.4114, 1040.6660],\n",
      "        [ 470.7857, 1010.8890,  500.4447, 1046.0408],\n",
      "        [ 484.0308, 1001.6966,  513.2944, 1036.7247]]), 'labels': tensor([2, 2, 2, 2]), 'scores': tensor([0.0654, 0.0547, 0.0541, 0.0507])}]\n",
      "[{'boxes': tensor([[327.4055,  93.9242, 368.0537, 162.9154],\n",
      "        [319.7820,  72.5147, 363.0640, 144.4575]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0571, 0.0561])}]\n",
      "[{'boxes': tensor([[562.2328,  92.0947, 602.3507, 159.8976],\n",
      "        [554.8741,  71.5664, 597.3007, 141.2678]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0529, 0.0523])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[479.4718, 863.4419, 504.8864, 894.0701],\n",
      "        [ 38.9711, 865.2986,  64.3150, 897.4760]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0611, 0.0547])}]\n",
      "[{'boxes': tensor([[487.7024,  70.1626, 523.6285, 132.6376]]), 'labels': tensor([2]), 'scores': tensor([0.0501])}]\n",
      "[{'boxes': tensor([[492.5511,  73.5961, 524.6800, 127.8276],\n",
      "        [486.7305,  57.3041, 520.6082, 112.7991]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[494.4713,  60.7634, 522.9772, 114.8555]]), 'labels': tensor([2]), 'scores': tensor([0.0526])}]\n",
      "[{'boxes': tensor([[ 41.9409, 791.4832,  68.5237, 825.0829],\n",
      "        [558.8165, 791.4957, 586.1995, 824.4245]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0605, 0.0508])}]\n",
      "[{'boxes': tensor([[617.5270,  92.2839, 657.8073, 160.2850],\n",
      "        [610.2294,  71.8552, 652.7025, 141.4407]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[ 37.8601, 755.4368,  64.2247, 789.6392],\n",
      "        [566.5637, 755.3702, 594.6027, 788.6628],\n",
      "        [ 34.3643, 739.1110,  82.1228, 805.7004]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0613, 0.0613, 0.0608])}]\n",
      "[{'boxes': tensor([[ 33.5239, 691.4512,  56.9358, 721.6279],\n",
      "        [501.7408, 691.3409, 526.4544, 720.7471]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0639, 0.0566])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[ 38.2511, 825.5536,  63.8832, 857.9425],\n",
      "        [ 32.5422, 795.0219,  77.3834, 858.9548]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0539, 0.0516])}]\n",
      "[{'boxes': tensor([[ 38.5854, 802.4769,  63.2260, 833.1804]]), 'labels': tensor([2]), 'scores': tensor([0.0644])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[374.5412,  67.9452, 414.5401, 134.1939],\n",
      "        [381.7045,  87.5465, 419.2337, 151.5373]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0526, 0.0511])}]\n",
      "[{'boxes': tensor([[547.2896,  99.0887, 590.4628, 172.0397],\n",
      "        [539.3702,  77.0012, 585.0283, 151.9956]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0529, 0.0523])}]\n",
      "[{'boxes': tensor([[580.7693,  86.7756, 618.6525, 150.7188],\n",
      "        [573.9064,  67.5663, 613.8514, 132.9986]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[530.2277,  79.2298, 564.8143, 137.6116]]), 'labels': tensor([2]), 'scores': tensor([0.0524])}]\n",
      "[{'boxes': tensor([[435.6303, 785.1577, 458.9550, 812.9075],\n",
      "        [ 35.4164, 786.7974,  58.4480, 816.0556]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0624, 0.0547])}]\n",
      "[{'boxes': tensor([[519.0031, 892.5429, 545.3880, 924.3353],\n",
      "        [ 40.2427, 894.5651,  66.3708, 927.7114]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0587, 0.0547])}]\n",
      "[{'boxes': tensor([[ 42.0902, 935.6333,  69.4180, 970.3007],\n",
      "        [495.3923, 933.8011, 523.4044, 966.6648]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0547, 0.0517])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[487.6730,  63.5306, 519.3247, 119.2207]]), 'labels': tensor([2]), 'scores': tensor([0.0511])}]\n",
      "[{'boxes': tensor([[410.1429, 866.0159, 436.3561, 896.9561],\n",
      "        [ 39.0641, 867.9747,  64.4672, 900.2518]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0549, 0.0547])}]\n",
      "[{'boxes': tensor([[ 53.2562, 657.3416,  86.9531, 699.8149]]), 'labels': tensor([2]), 'scores': tensor([0.0535])}]\n",
      "[{'boxes': tensor([[420.7165,  68.9292, 450.5107, 119.5611],\n",
      "        [415.1293,  53.2176, 446.8534, 106.0142]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0571, 0.0561])}]\n",
      "[{'boxes': tensor([[438.6846, 876.5020, 465.0203, 907.7548],\n",
      "        [ 39.5290, 878.4956,  65.1938, 911.0455]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0609, 0.0547])}]\n",
      "[{'boxes': tensor([[  72.8784, 1013.4788,  124.4522, 1079.1031],\n",
      "        [  73.4160,  953.3720,  167.1793, 1077.3627],\n",
      "        [1110.0780,  166.0052, 1182.4874,  288.3292]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0582, 0.0569, 0.0524])}]\n",
      "[{'boxes': tensor([[507.0111,  68.6370, 547.3993, 135.5586],\n",
      "        [514.2440,  88.4374, 552.1385, 153.0789]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0526, 0.0511])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[ 465.0027, 1271.2078,  502.7541, 1316.1366],\n",
      "        [  57.3212, 1273.8624,   94.5976, 1321.2329]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0624, 0.0547])}]\n",
      "[{'boxes': tensor([[  48.1160, 1068.6884,   79.4060, 1108.4291]]), 'labels': tensor([2]), 'scores': tensor([0.0547])}]\n",
      "[{'boxes': tensor([[  43.4470,  965.2094,   71.7012, 1001.1021],\n",
      "        [ 494.4732,   88.4554,  526.1924,  153.6423]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0547, 0.0533])}]\n",
      "[{'boxes': tensor([[ 41.2246, 915.2551,  68.0339, 949.2894],\n",
      "        [395.1883,  83.8772, 425.2848, 145.6904]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0547, 0.0533])}]\n",
      "[{'boxes': tensor([[499.6137,  67.6127, 532.4747, 123.3511]]), 'labels': tensor([2]), 'scores': tensor([0.0527])}]\n",
      "[{'boxes': tensor([[ 33.9089, 570.3840,  57.8392, 601.2970],\n",
      "        [513.4120, 567.9998, 538.6906, 597.9078],\n",
      "        [ 31.1030, 553.3369,  73.1809, 613.2919]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0593, 0.0586, 0.0582])}]\n",
      "[{'boxes': tensor([[ 33.5963, 671.5666,  79.3830, 736.9192],\n",
      "        [555.1899, 687.7086, 582.4342, 720.2620],\n",
      "        [ 41.7064, 708.9232,  67.8094, 741.7961],\n",
      "        [ 40.9267, 689.7547,  67.5656, 723.7100]]), 'labels': tensor([2, 2, 2, 2]), 'scores': tensor([0.0555, 0.0541, 0.0506, 0.0502])}]\n",
      "[{'boxes': tensor([[442.2247, 883.0251, 468.1899, 914.3479],\n",
      "        [ 39.8164, 884.9239,  65.7092, 917.8312]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0611, 0.0547])}]\n",
      "[{'boxes': tensor([[ 28.5756, 635.1470,  47.1588, 658.7660]]), 'labels': tensor([2]), 'scores': tensor([0.0547])}]\n",
      "[{'boxes': tensor([[530.2277,  79.2138, 564.8141, 137.5844]]), 'labels': tensor([2]), 'scores': tensor([0.0524])}]\n",
      "[{'boxes': tensor([[430.2749, 775.9377, 453.3394, 803.4155],\n",
      "        [ 34.9882, 777.6109,  57.7048, 806.4236],\n",
      "        [427.8294, 783.2895, 450.8488, 810.6385]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0629, 0.0547, 0.0513])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[ 47.6744, 588.5889,  77.8386, 626.6201]]), 'labels': tensor([2]), 'scores': tensor([0.0535])}]\n",
      "[{'boxes': tensor([[ 47.2638, 583.5583,  77.1687, 621.2643]]), 'labels': tensor([2]), 'scores': tensor([0.0535])}]\n",
      "[{'boxes': tensor([[ 42.5878, 946.3463,  70.2385, 981.4110],\n",
      "        [459.1417,  86.7218, 490.2284, 150.6314]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0547, 0.0533])}]\n",
      "[{'boxes': tensor([[ 36.1842, 645.4904,  60.3835, 676.0149],\n",
      "        [ 30.8332, 616.7903,  73.0392, 676.9150],\n",
      "        [516.4437,  77.2045, 550.1309, 134.0945]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0555, 0.0539, 0.0524])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[ 574.1812, 1283.5399,  612.1430, 1329.1525],\n",
      "        [  57.9505, 1286.3514,   95.6361, 1334.1859]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0583, 0.0547])}]\n",
      "[{'boxes': tensor([[551.1845,  70.9237, 584.4705, 134.0617]]), 'labels': tensor([2]), 'scores': tensor([0.0526])}]\n",
      "[{'boxes': tensor([[ 40.9947, 954.2517,  69.5425, 991.2958],\n",
      "        [613.4764, 954.1796, 643.8372, 990.2383]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0613, 0.0613])}]\n",
      "[{'boxes': tensor([[566.0663,  84.6145, 602.9904, 146.9647],\n",
      "        [559.3770,  65.8834, 598.3108, 129.6862]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[ 34.5992, 842.4353,  81.6068, 909.4993],\n",
      "        [571.5092, 858.9509, 599.5719, 892.4027],\n",
      "        [ 42.9475, 880.7988,  69.7821, 914.5622],\n",
      "        [ 42.1368, 861.0698,  69.5251, 895.9387]]), 'labels': tensor([2, 2, 2, 2]), 'scores': tensor([0.0560, 0.0553, 0.0512, 0.0510])}]\n",
      "[{'boxes': tensor([[ 497.0167, 1113.6918,  530.0915, 1152.8854],\n",
      "        [  50.1784, 1115.9744,   82.7571, 1157.3243],\n",
      "        [ 493.5738, 1124.2091,  526.5590, 1163.3014],\n",
      "        [ 508.3047, 1113.9865,  540.8509, 1152.9410]]), 'labels': tensor([2, 2, 2, 2]), 'scores': tensor([0.0654, 0.0547, 0.0541, 0.0507])}]\n",
      "[{'boxes': tensor([[545.8497,  81.5606, 581.4549, 141.6606],\n",
      "        [539.3995,  63.5057, 576.9424, 125.0055]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[ 27.3909, 643.6035,  46.7043, 668.5687],\n",
      "        [414.5527, 641.6528, 434.9450, 665.8657],\n",
      "        [412.4105, 647.8614, 432.6923, 672.4668],\n",
      "        [ 31.1711, 657.5240,  50.5982, 681.9367]]), 'labels': tensor([2, 2, 2, 2]), 'scores': tensor([0.0581, 0.0577, 0.0525, 0.0503])}]\n",
      "[{'boxes': tensor([[421.3797,  57.1792, 448.8521, 103.8919]]), 'labels': tensor([2]), 'scores': tensor([0.0567])}]\n",
      "[{'boxes': tensor([[  46.4763, 1033.8397,   76.6548, 1072.1453]]), 'labels': tensor([2]), 'scores': tensor([0.0547])}]\n",
      "[{'boxes': tensor([[406.4456,  93.4361, 451.3233, 169.7684]]), 'labels': tensor([2]), 'scores': tensor([0.0567])}]\n",
      "[{'boxes': tensor([[379.2306,  57.6541, 407.9442, 108.1946]]), 'labels': tensor([2]), 'scores': tensor([0.0511])}]\n",
      "[{'boxes': tensor([[533.4787,  65.5143, 572.0546, 129.3922],\n",
      "        [540.3871,  84.4146, 576.5812, 146.1148]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0526, 0.0511])}]\n",
      "[{'boxes': tensor([[376.8983,  51.5988, 402.5830,  96.8315]]), 'labels': tensor([2]), 'scores': tensor([0.0511])}]\n",
      "[{'boxes': tensor([[471.5118,  73.6944, 503.6695, 127.9976],\n",
      "        [465.6859,  57.3807, 499.5940, 112.9489]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[431.6599,  77.6777, 468.1324, 146.8286]]), 'labels': tensor([2]), 'scores': tensor([0.0526])}]\n",
      "[{'boxes': tensor([[609.2790, 116.4711, 660.0958, 202.2968],\n",
      "        [600.0728,  90.6877, 653.6556, 178.5137]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0524, 0.0500])}]\n",
      "[{'boxes': tensor([[  51.4191, 1141.8376,   84.8580, 1184.2982],\n",
      "        [ 535.7132, 1142.5164,  570.6126, 1183.1857]]), 'labels': tensor([2, 2]), 'scores': tensor([0.0547, 0.0536])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n",
      "[{'boxes': tensor([[617.3457,  72.6027, 651.3897, 137.2374]]), 'labels': tensor([2]), 'scores': tensor([0.0526])}]\n",
      "[{'boxes': tensor([[ 29.9015, 626.6888,  49.8990, 651.9256],\n",
      "        [ 25.4796, 602.9602,  60.3577, 652.6699],\n",
      "        [421.7427, 619.5521, 442.5796, 644.1016]]), 'labels': tensor([2, 2, 2]), 'scores': tensor([0.0555, 0.0539, 0.0519])}]\n",
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\spiro\\OneDrive\\Υπολογιστής\\Thesis\\Project\\master_thesis_dtu\\notebooks\\model_test.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m imgs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(img\u001b[39m.\u001b[39msqueeze(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m imgs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m pred \u001b[39m=\u001b[39m make_prediction(model, imgs, threshold \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\spiro\\OneDrive\\Υπολογιστής\\Thesis\\Project\\master_thesis_dtu\\notebooks\\model_test.ipynb Cell 25\u001b[0m in \u001b[0;36mmake_prediction\u001b[1;34m(model, img, threshold)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_prediction\u001b[39m(model, img, threshold):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     preds \u001b[39m=\u001b[39m model(img)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(preds)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(preds)) :\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:105\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[0;32m    104\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrpn(images, features, targets)\n\u001b[1;32m--> 105\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroi_heads(features, proposals, images\u001b[39m.\u001b[39;49mimage_sizes, targets)\n\u001b[0;32m    106\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m    108\u001b[0m losses \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torchvision\\models\\detection\\roi_heads.py:762\u001b[0m, in \u001b[0;36mRoIHeads.forward\u001b[1;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[0;32m    759\u001b[0m     matched_idxs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    761\u001b[0m box_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox_roi_pool(features, proposals, image_shapes)\n\u001b[1;32m--> 762\u001b[0m box_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbox_head(box_features)\n\u001b[0;32m    763\u001b[0m class_logits, box_regression \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox_predictor(box_features)\n\u001b[0;32m    765\u001b[0m result: List[Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]] \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torchvision\\models\\detection\\faster_rcnn.py:301\u001b[0m, in \u001b[0;36mTwoMLPHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    299\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 301\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc6(x))\n\u001b[0;32m    302\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc7(x))\n\u001b[0;32m    304\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#make a single prediction based on the train\n",
    "with torch.no_grad(): \n",
    "    # batch size of the test set = 2\n",
    "    for imgs, annotations in test_dataloader:\n",
    "        imgs = list(img.squeeze(dim=0) for img in imgs)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        pred = make_prediction(model, imgs, threshold = 0.1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "images , targets = next(iter(test_dataloader))\n",
    "imgs = list(img.squeeze(dim=0) for img in images)\n",
    "predictions = model(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[478.8079,  96.8675, 521.0992, 168.2464],\n",
       "          [471.1464,  75.4242, 515.7393, 148.4656]], grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([2, 2]),\n",
       "  'scores': tensor([0.0524, 0.0500], grad_fn=<IndexBackward0>)}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target :  tensor([2, 1, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Target : \",targets[0][0]['labels'])\n",
    "plot_image_from_output(images[0][0], targets[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction :  tensor([2, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJEAAAD8CAYAAAB3skanAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABMzUlEQVR4nO19aaxs2VXet2se7/SG7vZAuoEWYEWK7LTByFFk2YGAg3B+GGQHgQMtWSgmMYNEbOcHUpIfRrIwJoqsOAFiIgN2DBJgWUEdY4gigmMzeeiOcdMWdpv26zfdujWPOz+qvl3fWbVP3Rrue33rvbukUlWdcZ+911nrW8Ne23nvcUEXtA1lXugGXNDu0wUTXdDWdMFEF7Q1XTDRBW1NF0x0QVvTBRNd0NZ015nIOfc9zrkvOueeds69427f/4LOntzd9BM557IA/grAdwF4FsCnAbzZe//kXWvEBZ053W1J9O0AnvbeP+O9HwD4TQBvuMttuKAzptxdvt+LAXxV/j8L4Dv0AOfcWwG8dfb3769z8Uwmg2KxiPF4HLZR0ubzeQyHw/B/MpksHHPhvV9O3nsX2363mehU8t5/AMAHAMA5t9aoHhwc4KUvfSm63S4GgwG893DOoVAo4NWvfjWeeeYZNBoNTCYTjEYjTCYTeO8D03nvMRqN4JwLx2QyGUwmE4zHY2Sz2bDdOYd+vx9+j0ajcD9em9e81+luM9HXALxU/r9ktu1MKJ/Pw3uPTCaDXC6HyWSCyWSCwWCATCaD8XgcBpsDzcHmt3MOmUwG2WwWhUIB2WwWuVwOpVIJ5XIZ2WwWmUwmXG8ymaBQKKBUKuHo6Aj1eh2TyQS1Wg2ZTAbXrl3DcDjEaDTC17/+dYzHYwwGA9y8eRMAMBqN0Gg0AACDwQCDwSC0R6Xleaa7zUSfBvCoc+4RTJnnTQD+2Vld3HsfpIsSByObzWI0GgUGIhPY451zQaqUy2Xk83nk83kAwHg8xng8RqlUwmQyQTabRTabRa1Ww8HBAUqlEiqVCr7pm74J1WoV2WwWlUoFw+EQ3W43tJHMnM1m4b3HcDjEzZs30Ww2MRqN0Gw2cfPmTXQ6HZycnIT2t9ttnJychO29Xg8A0O12E5L1bkrAu8pE3vuRc+4nAPw+gCyAX/Hef+Gsrp/JZHifRCdy4HQ/Ozymdnh+pVJBuVxGsVhEJpMJx+RyOeRyOTSbzSCl6vU6Dg4OkMlkUK/XAzabTCbodDro9/uJ+xwcHAS1e3h4CO89yuVykFrAVDKRqR588EFUq1VcunQJ3W4XTz31FI6Pj9HtdgEAJycnaDabuHXrFhqNBo6PjzEYDNDtdgPj93q90CZV4doHmzDfXcdE3vuPA/j4nbh2qVRK/Kdq4qBQmjjnEsfwWxmrXC5jb28PpVIJudy8myhZ+LaPx2N0Oh2MRiOUSiWUSiW86EUvwv7+flB72WwW/X4/XHs8HiOTyQRpdnx8jAceeADD4RC9Xi9IuHw+n7g3gPCfUpLbKpUKnHNBSgJTqTkcDjEYDIIE7vf7CVXJNlH6qnrX361WK7Xfzx2w3ob0TVJGYYfk8/mg0kg8TvFHtVrF3t4eKpVKwFaKk0qlEm7evJlgQGIeHtfr9QI24/5CoQDvPfL5PMrlchh0DvTh4SGcc4Epe70ehsMhhsMhAAR8pio4l8thb28PrVYrqEsACWyl0oXPrtsoofnS6XORuTqdTmq/31NMpIyjRAbJ5/OpxwAIKqVer6NUKoUOJZguFAool8vw3mMwGMA5F1RbsVgEgADALYgH5niKOCiXy6FQKAAAOp0O9vf3AzDv9XoolUoYDocJME/GozoiU9++fRv9fh/OOfR6vQXLkv1Aa9I+d6xfyER8jjS6Z5jIObegzqjK2AEcaL55JEqwYrGYUGEcODJKPp9HsVjEyclJYl+xWAxS7sqVK0FtUnKp6a/3V3BPoEz8BSDck6qL28mI4/EYhUIhuBsI1FXSKlEFsz0WB+pvZSpKwjS6Z5hIiWKZZjg7iCa7NZ0pFWq1WsAWAAIuITMUCgVkMpngMiCuqdVqODo6wuXLl/HQQw+hVCoFqw2YMm82m8VwOMR4PA5gfDweB8uv1+uh3++jXC4jl8thPB6j3+8HSURm0ufJZrMolUoYjUZhoDOZDIbD4cKLon2h6sqS7l8VaN+TTATM33bFAKVSaaGD2EmVSiVIIALbQqEQmIhSiAOWz+dRqVRQrVaxv7+PBx54AIeHhwHr8L5kWEoPtouqqtVqBWYZDAbI5/NBenrvUSgUUCwWUSgUMBgMEv6sYrGIYrGIVquFwWAQ1KRKIuI54sJl6pzXtR780865Z1JBiC+ox4G5uqDlRWehFd1kCFo+DJ9QSlCdZTIZ9Pt9ZLPZoHYymQwqlUoYUDKD+mxyuVzwBdFKGg6HQUIpY/d6vUQ7AKBWq4VrKBORYeikpAQjs+ixAKL+I+0vtkG/tR1pdM8wEZB8Y6h22Bnj8RjVajV4tRUbEETzjaWFlc/nkclkAjNlMhn0er3AiAS6BwcHqNfrKJfLCUbm28+B1pgeVSLVFKVXt9vFaDQKjEQGZ9yPjKO/u90u+v1+kKLKLBor1PuTYirL9s99y0SqUoCpWiuXywnwTSCqQFpVFyUQPxw0MlyxWAwOQPpsNM5GxqFvhoNMidFqtYI0IlajKU08xTb3+/0EsNZn4MvCF4fSiNJQwzzKIMuYZx2n4z2DiQqFQgLP0MTWYCpNdCViHg4aTXkF51RprVYrSKp8Po9SqYQHH3wQV69eDQBXTWh9g/v9PrrdbgD39APRRKeTkX4jMjifiepPnYS8DtUnj1Ppw/aSifRFu/2W25jspcfn3InD0QePwv3S6J5hIkoRqgZ2JjuNg7u3t5ewTigJiF3IVGQAdQuobwiYMmCtVkOxWAyMQAym6kAZi5KIXuThcIhKpYJmswlgak73+/2EKuV5AIKUUcOBqhFAQoWTNFaX2L43waX/cCn8V+bz3qPxU43QT0v7foXx2QnSgQMQYlUkdvb+/n5QTXzTNTzBwVNvs+IhNf8ZJxuPxyEOBiB4eHu9HjqdTjDTSVSPlE7EVupdphQpFAoJBqCpznbQo10oFIIksqrMWmxKVn1ZVXZfYaJisZgAtNrR7JjBYIBqtRpwEQeTRKaheuOHDKT+l0KhgMuXL+PKlStB8lG69Pt9jMfjYOHRg03AS4wEIGG2x1I/iK2opq111uv1AvOqJ1uBtHrPtT8sWWbiffR/jO4ZdUarC0BUdKv5XK/XcevWLeRyucAomkdEKaTqkXEp7qtUKnjkkUdQLpfR7XYDI3HQeSzNfAZd6QxkG1VqqDWn2IyRejob2TZG6dWkJ0PxmdkfGrVX9XT8z4+X4qKbP3wTVz50ZWnf3zNMxEFIE9300zjncHR0hK985SuJgCNVmzKPYqNOpxM6n1Lo0qVLYeA1+ElnJAeUjKrZkSqVyHSqeuhkJHOotUmJ2el0EhmclIR8XtKy2Ndkb4LD9x2Gc1QSnfz0yVIGI90z6kwBqHWykcGIQQ4PDxO4h8dycAAkpBOzERU3fcM3fENgQDIfJYUF8+p51rwkxTBsp6okBnvJ/GwrJVmv1wtWHSUcSa+TJpnTKM30T6N7hokY6ATiSfg0nXu9Xkga04xHAIlgq25vt9tBGhQKBRweHuLo6Cg4HTWWRnWjvhpto96n0+kEiaQebjIiJxfYHHCSpomoRUrSlGDrmT5LumeYSNWB9YeomqDHuVarAUBCjalk0U5vt9sAEAKuDLJ674OHmRLGWoWa562YiucpXmG7GZ2nClSAzGuQgQjggXiuUJqneh26b0x8Sg+NLwHJpDMOyGAwwNHR3ImmJr6qMfqGmKdTKBSwv7+Py5cvBwce33QCZ2IXTbsgZiGDeu/RarUWQhkaJGW8jMltzAKg6mK7+Iz9fj9YhRr2oES0FMspStt/2oSBe4KJmG8DLGbykWj1MOHr0qVLCetLk74AJMIQZDR6qOmT4TG8jsa0lHE0pQRAiHVpshpxD4CQozQej9Fut4Oao7OTao6qTL3aJGWiVUIYy6TNaeffE0wEJBO9gEX/huKNXq+H/f39hDc6xkw8djKZhClBly5dCvvoDKQprjk9GmsDENSl9x6NRiPhUWfbqJ4YwqGDlEzK69PTrUluaRKHqmwbPHRfqDOqGg5ujBR30MPLhHuNnfF69M/Q91Ov13HlyhUUCoVEsjsZg5KOTsfxeBwSzOgqoG+HIQ4yIRmCTF6pVJDNZoMPSK1Gnsd7UI1TrQKLmIjPtCndF5KoWCzioYceAoAFJrIAm4M9Go1w6dI0bqRORrWyGAsrlUrY39/H0dHRAkjm+TrrlbhmOBwGCZTP5zGZTGd2kFk0sk+mcM6hWq3CORfCLJryqhYXMMeCFodZy9DSOlH6tJAJ6Z5wNjrnUK/Xg1rQAKv6YIA5k/V6PVy+fBm3b98OxyoRs2QyGezt7QWTXnEPfVOZTAaNRiMMImNtSnQFEGNp3hKZGpjONCmXywGPUcKqn4r4CJh76tMcimnM4pxD5iSDyd4Et99+O3oM958mxe4JJspkMqhWqwFUq0VmJRGtqF6vF0AygMQcL4ZIiIX29vbC9WmpEfNouIHSids0LZVMpLNgqUbb7XYAzdVqNQRnR6NRkEqj0Qj5fD5IIRutp+Wm1mnM7aH9sf+r+7j99tsLHms+Vz6fx42fuHF6/285fueC9vb28MADDywV38RLCq6ZI83OJRMw2ApMU1P39/dDGIOOOzIRMPcFUUXpVG1tE+eTkZkpyXivQqGAg4MDZLPZkLtEtcjfxFXqVtCYWcw/tgnpdU6b7XFPMBGAoAJixM7XAaZJzfwiWkBUI5PJBMViEbVaDdVqNYQfACSckwASVpm+ySoVmMaqbaQZz/OZYuu9D+AbQJgOBCA4NKlq1WvO/co4mzKR0mnXuCfUGXFIzGHGbQxF8O2lWjg8PAwFE+gJppSoVCrY29sLjj5gKnXo86H6oJOP4FnNd/UhadEFAME6nEymkwj29/cD/qEjkTlJGscjE/F+iqlsTtKpltVJJhUTAUC2mb0/MBFxBDuMzKMYIJfLBVOaFhH9RWqVUY1wBgdzj3RAeD4w9whTzdBVwMGleW6j9ZwxwoILZFZaenRbkIkI1ilF08x7S6cx0cF/PQjH6UfTgo/Hx0uvsbE6c8691Dn3Sefck865Lzjn3j7bfuSce8I596XZ9+Fsu3PO/ZKbFvz8rHPuFZvee+EhZm8oKSbOmZKqlgyBq6ZcDAYDlMtllMvlMI2IzKcSRt0AxCfqa5o9c7g/K4OQuZiWS6Beq9VCPE1nuDIzksyqlplKVesbWgUPLZMwdyvsMQLwM977lwF4FYC3OedeBuAdAD7hvX8UwCdm/wHgewE8Ovu8FcD7t7h3gnRGhZKa+razyTDZbDZMOKSFVKlUEhafzhzVcAcxDzMeeR9u13gaHZdUqZy8OBgMcHBwEHANsRYZlxMbCaApdTSMo/EylcbAZphoXcfkxkzkvX/Oe/9ns99NAE9hWpPxDQA+ODvsgwD+6ez3GwD8mp/SnwA4cM49tOn9lcrlcphqYzuRv7Vymprck8kE9XodwDRVleVhOPvVZgUyrkXVovcjlrEBYEoPxWTVajX4gUqlUgDzNPv7/T5OTk4C3uJ1tZoamcbmJPE7DROdxiT2nLvisXbOPQzg5QA+BeAB7/1zs11fB/DA7Hes6OeLI9d6q3PuM865z6x6f4YIlpFNBuMb3O/3sb+/Hzq9Wq2GqUVqWpORiGUIsFXqqFfZZhWo2qtUKoFB9/f3g9+HOdl0AajaJRPRUqNU5LHqLSelqbRV1RzvcVoqydZM5JyrAfgtAD/pvT/RfX7a2rXkqff+A977x7z3j616DkErTW4F1CoprA+FFlOtVgtAlrNkgTlgZSdmMhkcHByExPtOpxNSOhQ0K6jmHHqGMDKZDC5fvoxOpxN8VK1WC6PRCAcHB8HRSNXX7XaDf0knA6j/iJajZQ7rfCWtouqUkU6jrZjIOZfHlIE+5L3/7dnma1RTs+/nZ9vvWNFPTqtRIAssdpJKCA4KJxAy+Z7z9ZU0zHH16tUgGXQGiHqF1QIjQ5IZWZKPyXGc289Zt2QSxVC0HGnaK4jXMIzFhKcB4lWB9Wm0jXXmAPwygKe8978gu34XwFtmv98C4Hdk+4/MrLRXAWiI2tuKdObrrG0AkviAg8qoOgeD4Hp/fz8U6lTwSkCeyWRw9epVjMfjUA+RKoaMo0FbSip1NmazWezt7eHWrVsBy9C/pNmJvA4lLF0E7XY7WIj0VanK1We14Y/I+K3Ut2kpJon+X+lKcXo1gB8G8Dnn3F/Mtr0LwLsBfMQ59ziAvwHwg7N9HwfwegBPA+gA+NEt7p0gPqQGPWOincymoQcOyqVLl3BycpKIr/EbmJaCKRaLAeiqea5WFxmK205OToJTkFJIA6n0OCuI15xuWmuDwQCNRiNIXV5HJVFav8QopuYsWcmeRhszkff+fwNIa8XrIsd7AG/b9H5ppHEsBlNn9wv7gbkkIDZSsDgajXD58mW022202+2EM1FnxHJ+Gb3ftKRs/jT3cUo0t5dKpWAB0m/FkAqlI3EPE/l1+jZVJ6Vps9mMWmCqUldhltP69zTaeY81J/nR96Kdp6Q4Qj29vMb+/n7wtRAU0zfD7EWmZeTzeZycnCRmeeTz+ZA2S0alpOj1esGiGwwGqNVqCf8T73lycrKQcqsMwXbxOzZRUVWZzWJQWkUKAcvnrJF2nonUGqpWq2GbJWYz0tcDIAwWfTUaOuA1mAFJtUOzV52OOodN3QeUeJzMSIZgOgj3dzqdUCCL19c4G/GZLS3s/TRlxXqrtV+UNpFIq6iznY/iayYipwHpg2s0Xbcr1mEZPgXmOgiMo3EwmSRP9UMAzHnxlGCUJsRLOv+LfiZaZ5Ry9Cfx2sDc1UC1yVQR7ksD0Ks4DZdJKusWSR2DU48450Q1QicekF5rUHN+yEQMfFJNaWiE6ghAwCGUDM1mc2GKETCfUEjJo7nT9DbzHgxvaAaCcy7Ey8hYzNUmJrJ1ibg0Q4xieOk0UkvvNEcjcA8wEYmJ8epwtJJHPdbEMZorxFweGxfjnHcObqPRSMy40FCKJt3zGurf4Tz7y5cvh3brzI3RaBRSP4h7WIiCkkqT0QAEK5Gk3mzdFrO2TlNXp5n3wD2AiVQ9aN0fewwBqb5dDDN0u90QudcgKqVOu90OUXcAoY51q9UK6RK0pDg7hK4AOg41ReT4+Dhh8tNpSVcAMRsnLtKk5/nEQWRIC7BjTKR9Yf+nMdKqGGrnJRFFvZrYSuxUTUojNiGGYvK8WneKCSiNer0eut0uut1uKPtLCaW+KlpkLCABIDgU6Ti8fv06/vZv/xaZTCYx/00rt6kjkc9JCcQccE1IUwvNBoG1P06jWNhoGe28JCLxoS2+AOZTglSd5XI5VKvVENMajUYL9Rx5rkoTMiuAYKqTASkB6CxUy42ZiOVyOcTbOp0Out1uWEGIg0/LjiqsWq2Ge/Z6PTQajVDzWqWQMk6aGrLShS9MjFko7U6jnWci7RRVBepkY8agpoKwcGe32w0+HBZRAJDAHBz8er0eindSqgEIZjp/Awh1relTYluZjM/5Z41GA845XLp0Ce12G81mMwDocrkcgPjNmzcxHE7XTCNGosSzpLguppIsftpWne08E/EtpSmtabIky0Te+3Ber9cLVhmliSahFQoFHB0dwTmHdrsd0lm5n6oRQLgWgTK9zVRlGt2/cuVKCGU0m80w/40SR0G6BnvpH6IU0/n8Kmn5AqwTZFWGWsentPNMpNglJsJV/ShzVavV4IfhYDFAq0lonEVCk7vVaoXt9JZroJZtomOQIQzvPWq1Wkj7IFNrnaTBYBBibrTm+J9SSAG79czHnI7LaJmk4rVWMfF3nolIaUwEJMMc7LhisZgIR7Az2YGKsRi70kEluC2Xy+j1emGyAFM8KCGYZkKsVqlUgstArwdMnZoMxtIv1O12cevWLUwmk0QBUX1uEqUs1VmaNFHDYVl/8tjTaOeZiAll6gi0aRFWxXFQOVdeHZDKcAyFEC9peV9NRFNwq7M1uJIQmZbBWvp1GE7ROJd6vlutVgjJMM+oWq0mlgK1MTO2fZkkSlNjmwZrd97EJxAmE8U6j0ykTEV8QtLp0Wqu62+m01KC0EdDvxIzEnkO/VacT8bpP1SDvP9gMAhgvd1uh4JVZByqZIZS6BJQ0pfEzv5YdqySSmMed1/Ezkg0h61JqgNAJtNin8C8PhGlESWaSiidbcHr6n4CXiaUUQpx5gjJZgVw+pBWguU16FPSl2M8Hgd/E/8rkOY9SGlqKyaN7P+0l9LSTjMR1QQwr/ZK/AIk55ypiOd+LuSrs0dVatGXw9+apMbB1lCEDqwmllFCaU40mY4Sh05MnV3LZwKQYBwSn8dioZhvx5r1sd92230VgAWQiJgrURJZsUzMoyEO5hxRHRDraNRerT29P5mMfhtlJEbeeS+dx09znc5OZWAAC+VoqFJtno9KyHXM9GXqKi10YmnngTUljOZN286zkw1VTCsgJxPpG65kZ57quqvKTLoSNS03Xa6KTkQaBPSoM9GN6taGc/iS0FdF0sFeFnlfFzTfF0ykiWDEQ5rmQWLnW6tFZ3XouWoicwEXqkQF6JR+yoR0UNJKu3XrViIgq6qRDkQGjZmOwuszXseXRKUWMM8zUnV8GqhelVZVZcCOMxElgGYAxjpQpQuJg68xNZVEBNNAMpqu+IO+HGAeaiDusYO5t7cXmJgzPOg8BOaeas4+odqiJOO1bXkaYDFmtgoYTqN1g6/AjjMRiQMTs8yApAfW+kO4z06ApBphpzLPhyqNkorXsSCX+1SCAFOHooYyxuNxSLi3Na4Jvmn9MVtA1YxlGPWar0LLYmerBF+BHQfWClb5lqdJIrtdHXzAXDWoitMEN56j6kzvq/tVGrB9rVYrBGL7/X5iEWH6oDqdDk5OTgI4J2NlMplQVUSvaesEWDy0qld61e1ptNOSiOYw8Y7NJSLRB6TSSHGTMlIsjECm4IDqikJkAq1bzTwjzvEnozHU0el0wlx/51zITSIp1qNDs9lsRsG0zbGOgeplTHEW+GmnmQhIVsaw9Z51oCl1ONAkMgFVIq+hFhwxUr1eD6mxVHWsIML/VHs8V5dSoAXHde5V5QEIXmstXTyZTELhLftcZBht5zp46DQGui/UmaobTY5XtQMsqjPNKyKp+Q/MXQcE05Q0WiuoWq2GAhC8x3g8DlVFNERCEF+pVHDlypVwXWYFAMn1WzkBodlsJmo28nlVpZKWWWZpTsXTHI+r0E5LIuZEAwi5zVRvJHaUShbrT6K1ZD3aCsLtTNdqtYparZZIyVCVRFOdYJ3BVgXJdAeomtQ5cQTfNkOBzKpMtMw6VTpNta1j2pN2WhKRqEYYQNW3VONeJEoBAIli5FRpypiKN4hh9vf3sb+/H6wrYK5aNN9ZJyxSalHtsd1kLGYWMG233++HCrJkEOv/UqLE43U3YQalZU5LSzstiVRicPCJcfhGxhLSKCEIyHk+86D55muqLd/yUqmEcrkcmIX302LkHPBKpRLmwmm1Ec7WoLrSwusAgqSiGqM6BOaqOM3aXKffTqNVQfdZFLnKOuf+3Dn3sdn/R5xzn3LTAp8fds4VZtuLs/9Pz/Y/vO29OUDAXAKo2U9JZJmMA22daqomrMXD2Jma36pSdO68gmpWoKXnm9uZSlIqlXBwcBCwFacicWKiltdTLKdt5DW3cTKSNpFiZ6HO3o5pvUbSzwN4r/f+mwHcBvD4bPvjAG7Ptr93dtyZkGIDtXaAJKjW46yPRQfIDpL3Pqy3Sk+yXpP+G11RiBVK6vV6qIBGicXfZKpmsxmyAQCE3GkF+8rwsdQPa0lto8psX51G21ZKewmAfwLgv8z+OwCvBfDR2SG28CcLgn4UwOvctk+KZFEnaVeQODarUR2H6qC0EijGYKqyNH9IsYPiMJYwBhDcACzBxxQRTQ3hfH8Sc65tN9nnsd56a3Ft0s13ExP9IoCfBVCf/b8E4Nh7zyfS4p6h8Kf3fuSca8yOT6xA4px7K6Ylik8lTjYE5r4SAmV2nAJtiyWslWNjXoqrqKaYG6TFz5ngT7XGzmfMi0xDh6HGyYrFYkL1MjVWV5lmW2LSgUaFSuJV6DST/66oM+fc9wF43nv/p5teI0Z+jcKfmhLLjrQxMQ3QkihZFFSTSXS7SiuVYM1mMxFXI9jWeo+MvFPS3L59O4Q1CKaHw2FYpJgpJPzPa1iKBV/VKlPaVNAvi6fFaNtye9/vnHs9gBKAPQDvw7Q+dW4mjbS4Jwt/PuucywHYB3Bzi/sn1JG+zcDcKtOcaRJNfnY+r6VTeaim7Dz4yWS6gAwtL13zTPOL6HRksFWLP3B6NbMQyIztdjts00mJqnKBxXzqO2GVrXPdbYqhv9N7/xLv/cMA3gTgD7z3PwTgkwDeODvMFv5kQdA3zo7fOHCTyWRCequKeRs01WnVymCcQUoJRpNf0z1UgqlKm0wmaDQagZFUpUwm8wVkGo1GggEUu6gDlKCaDKuzcPVZYpSWiLcpbWKd3Qk/0b8G8JvOuX8P4M8xrTCL2fd/c849DeAWpoy3Mak/SCXHKpKI05s1aKoeZ2XMtLCApo4wS5EgF0CYkkQppEuTa+0i7+epHzxP1y5TqaDYTS3HF5rOhIm8938I4A9nv58B8O2RY3oAfuAs7keyjreYV9cmpNEFQFMcmKs3e5wyDTEOBzaTyaDZbIY8bc7w4HU1oY1Tnlnwk0zFXGlW/Jj1U1BlZGJeTwG/tSoVC56FVFo1+ArscNhDK5fxw3nuQPr0aoZGODCMX2kczUoiDp7eL5fLodvthmqznAJEVcZv1olkztDt27fRarUSWY3WsrQBV+uOIK1ihitDxaRWzNpLOzaNdpaJyAA60HTkkSh11A1AKcRB5rUsmOQ1VYUo41F6NRqNEMYg1qLVRSditVoNKzxaE1o93c5Nc4ti0sQyD59HLUel05hhmUvgPGCiu0Y26s6ZpCQOujIHpYaa6JypqhLBruSjGEzNfc68IOMQZ3HWBnOk+aHE5JodvF4ulwvYiPfiPRTcx6L5yyjN8biMydjGVWlnmYhSRE1qFmjQKcex8zQTkecXi8Xgo1FcpcBdrSVKJg4GQxqcocppQUyuZ4kZ4iLWi2SIg1F7Bc4WA1ncpo7SNMmxbB+fJabS7paf6AUlXQmI03m63W5It9AovBKlDtUaB6ZQKKDdbifMZe1M+2ZmMpnE6kQKiDUOpkUheB6tMhbPajabOD4+Tpj/um4HzyORga0jdJlaW4U2BeQ7y0RAUp1RtRweHgKISxNu1xmo3E+cZONgZDKmt+7v7wcXAVUncQ0Z+ObNm4nUDwJ+Xo+SKJPJoN1u49atW4lC6JQeqja1Tdy2bj71MlpFzaXRzjKR4gMFxZQwABJmMxmBDES1pqCWJVzUqUhmKxaLeOihh4JVRTOdFT/IHIVCAVevXg3J9bqUFbGQpoc0Go0QzGU7YqBXVZwy6CaUxmjKQPcFJmIhh5i/RL3ViiPIPOonUitPpzTr9kwmg729vRCy4P1PTk5wfHyMw8PDIJWY9krsMxqNAmBWgN5sNtFutxOOSlIaI/F5gXl25qqSZ5Xj9J73hXWmDjdN3yAzTCbTUnYKPpWJgPkERM43o6TS5RmA+ZKdOhWaOdPOOdy+fRsHBwehcgevyTZQIrLmY6fTQafTiYYrVF1ZUsmoOMvSJipN447ajlVoZ/1E1kcEzHECB5exNQ48JY+az2rB0dSO4QO1xLhefT6fDyGMRqOBRqORmGBosRfVmnUmktQa0+fhb421KXY6q/7kdxreSqOdlUTMR1ZfCWNXXM6Alg+JgFazD4mtyCjKbLw+gBBY3dvbC1X2Dw4OwrW1FB9LypChtRgESxM3m80gjXgf65GO5Tpxeyyib2kdVafXWZcxd5aJ1HejuURcpC6bzaJUKoVIunMuqBbFQzaYGetApo1Uq1WMx9Oa0hxwrYTW6XSCg5EedGYb0HlJ6UQQzsmQ6iG36S2qstOYJtbudSXVfWniA8nCCtlsFu12O5jj9N/QD0QG4kBSUhBH6bogKt7H43FwRF69ejXMB+OxpVIpLE3OIKxzLlQ+KxaLYQmIWq0WKsyS2W/cuJGQMmwbkAwOq9RU6ZsGwpf9X+ZQtCm/p9FOMhHxDpAEm3TyKUDW1Ax+lGwUXBPV9H5MhT0+Pg5rpAEIzHT9+vXgLefyV7wvgODB5sBxzddisYh6vY7j4+NETI6kUsi6NE7rI0tWMlkGXNcqI+0sEykpyGTCOyPzGoGntKL1puB8MpmE8AOP4RLnmr3IMjD1eh21Wi2kcVC6eT+dqFgoFFCv18OUn2azGfxECpABLHjW6aexQWH7vNv2G7elRfJXpZ1kIiUbw+KcfLWyKJlssQY7tUitHgBhXlipVAozNzqdDm7cuIGTkxO0221UKhVMJhPs7e0hl8vh6OgI4/E4BGAPDw+Rz+dxfHwcgLQuV85cI72vlUjERXyWmAd7VTotjgacXsLY0k4yEfELsFi0im+ulUTEHxwgzUi0y1gp/nDOhWApF3eht7nf74ccIu89bt26FYpZsXQMLUJWRmu1WmHuP5nIThQAFkMdGgJZhmc2UUf2emn+pzTaSSaKxXfoHSYzMDYFzFNCyECqLrgfmFt8WveIx3IJBgAht5ozOLRyyGQySaxszUmPAEK1kHa7HZaboqsBQKKgJ90UMatM1bdlJrtdHaaxnG31TW2Ki3aSiUhqAquY50wLxRo2bZZvvuZp6xtIJtKEtdFoumRns9lMLEXFlA+mepDJOAOEQVia+VSTJycnYUlQGgu8T6xNfI6YFIpF8vW3vkh2n32Z1qWdZCKCWK2aSlOcnU9pQumjnacDZD3eOvGRqpHSjYNNsKxJ95Qm4/E4ROXJUDyfUkhzjur1emINWL13GsXyqmNkrTFlvmX71g3s7iQTaUIaME+VVV+LMpGeQ+bRMr4xlUHMNB6PwwpCwLzEHxPQKF2ownhPTlTMZrOo1WoBxzERjct60vqja8E6P60nOyad0iiNSbgvxojL8FYa7SQT2Y61DjlgbjarWooFL1WEW/E/mUyCb0dNcroC1FOuzkwS29BqtYLUUvxFicb22HpIapUprQN8lVGsEWKf+b7yWOuERGA+S0IxDkGuztAAkDgmFhYgw3CwyUQMqRBLaaDSThliG1UyaYiGILdSqeD4+DjkHfFF0GCrMgzPX4WJrBRKA9B8Fk19WVed7WQU3zrnNLEeQIJ57PQg7qdK0zgViR2tg87rcDvxkao0Zg1Qqqi3mYxL7HRwcBAKpNtov7bD0jJ1toxxbEyOx/NjMxzWoZ2URMBiZVj79mSz2VBYyqZ/6Pn2mwzIzrUSi4FcSiViGWUyesY5cKzARoDN4PDt27eDh504SpnPRvHTJOgyNWQZicdbBlLf231h4tscIJrQ1i/EAVMzHUACy6iXWgeI500m02xGrgDEfGliGvUP8R4sJQzMJ1mS4ZxzODg4QL/fx9e+9rXAtKxXTeZMk0LWEEgLslrJSkas1+vY399PBHPpMdeio+vQTjKRzi0jHgLmHUhm4qDpnDLtINtZaVNw1OzXcIkCbTIsrTaNi9ERmsvlUCqVkM/n8fnPfz5kBqQ5PWMxMmvek9IciDaU8vKXvxyvfe1r8eCDD4Z5cM899xyefPJJ/OVf/iVu3Lhxf1lnOogkBcUcTK3iGvOPUPJQCgDJ+JUtFKGJZlyNmim0mjxGnxJTP7z3uHz5Mp599lk0Go2Eo1QLgsYYnm3hi5EmrdIckZTOly5dwsMPP4wXvehFODw8RLFYxLd927fhsccew5NPPomPfOQjeOaZZ9Yaj23L7R045z7qnPt/zrmnnHPf6Zw7cs494Zz70uz7cHasc879kpsW/vysc+4Vm95X1ZkFxSqWyVzEHFQ3GmBU8W/Na60aotdnvpK2QUMmGmvj/tFohHq9jna7ja9+9avBGuOzEA/pNfU31fZpGY1pYQu2uVgs4ujoCPV6HZnMdM2QfD6PBx98EK985Svxmte85q5jovcB+B/e+ze6aZXYCoB3AfiE9/7dzrl3AHgHpuVmvhfAo7PPdwB4/+x7LVLvMK0j27F0AXA7lx1XUslCUj8NpRMwlwga9GVCmj2WKs17HxLjGKjd29vD5z//+YWaQlw8WJ+B0k8lrj5TWt/oNXQ7JRHrApTLZfz6r/86PvvZz+Lg4AA//uM/jgcffBCPPvro2ky0Tbm9fQD/ELP6Q977gff+GMkCnx9EsvDnr/kp/QmmFdUe2uTeaclZxBC0hChBdNkEi4PUR8KB5TadcMhjCODpOFQQr9cmI1FFlctlXL9+HY1GA8BcLQ4Gg5CTrWrM+om894lpR2x7bMDTmMA5F7znit/a7TZGoxH29vaia82eRttIokcAXAfwq865vwfgTzEtR/yA9/652TFfB/DA7Hco/DkjFgV9TrbBrVD4U8MeMbHOKqxa21rjaDzXVgWx6RfMP6I0YMFPMhUZKpPJBDxElaTHcREZWmP0UudyucT8e2staltW9Q/xeLufH9ZTAoAf+7EfC2X/rl69Cuccnn/++Wh25zLaBhPlALwCwPu99y8H0MZUdenDeABrQX1/SuFPhg+AeIaf99MZFXYeOwGwDpa+0Ro2Ib6hZUcppMlulGwckHw+j0qlgkKhEDIr2Zb9/X3cuHEjxMt4frvdDkBZk+Ji0hLAqXjoNGI/8D6VSgUvfvGLQyLdyckJnnjiiYSqXum6G7VmSs8CeNZ7/6nZ/49iylTXqKZm38/P9rPwJ0mLgq5FMeZR4qRFVXXaecA8aDtr54JXl0yk5jzPJXjmLFy9PuNq9EQzFPK1r30ttJ3XZD644jIrDe1nU6IaVB/WV77yFTz11FNhHlyv18Px8fHa196m8OfXAXzVOfcts02vA/AkkgU+beHPH5lZaa8C0BC1tzJZ0R0T8XTccZ8mqWnoIvJMCf8OMJ+uTYZSy46J+WQoxS9UddVqFdevX18oW0MHn0pHPo+1EvkMqzJRDNPwXPrVGo0G3vOe9+Bd73oXPvzhD6PVaqFYLOLRRx9d6R5K21pn/xLAh2aW2TMAfhRTxvyIc+5xAH8D4Adnx34cwOsBPA2gMzt2bSIzLOvUfD6fWLiOzFGpVALQVVWkOIS/NVDKa/DYTCaTmBhJTMXjyVT0DF+7di3sZzIarUX1aQFxKRtLVVlGMRVPFT0YDHD9+nUAwPXr13Hr1i088cQTeOUrX4lLly4lCsyvSlsxkff+LwDEsMvrIsd6AG/b5n7metHOInahtaQMx+nPatVpOT6qNVv3kTjIWkWUTkzmV0uPBdKZoE9rbzKZ4OTkJAqk1dOs96H0sI7Sdawo9kGz2cS1a9fQ6/Vw69YttFotfPnLX8Yf//Ef47HHHgs54uvQznqs00g7nQPFAdK589b64bnKLPTV0KJRtTiZTALAZ10itfCo6hhGoJshtox5LHWX7SLjxKxHPdaer+fx+NFohGeeeQY3b95Eo9HAjRs3Qh/90R/9EYbDYcBu69DOMdEyPwYHiz4f9bdQ8hSLRXS73YVraD4NpQOZqFwuBybhMaVSCa1WC8A0cb9YLAYpt7e3h1qtFrIb6XB0ziUW2tPAsPWkA8mZHTEGWoX0peh0Ovi93/u9hBOT9Qg+97nPBQZbl3aOiRSAKiPwrSUuoSTihwNHlabnMHjKbWrBAdOpz/yttYuY1qqBVTKUc/NVpgnovfcJgG0lnz4jMZua9acxj3VEWhzH/+w7VeXAvOb2urRzTAQsfxOJO6wpT3HOGRoadtCUWjradOA1r5ozXmldlctlXLlyBfV6PSTz01w+OTkJbSUQjy3mElOtQNzcX4aD0lSb/eh2vee6kxZJO5fZyLlfMSIoVmlCL7KmlWp6rYJsnssOZpRe5/2T+cgMh4eHqNfrId1DVwtqtVqJCQNM/LeecwXWaolpaoi10GISLOap1mP5sZmeeu91CjmQdk4SaQRfiR1oS+wByWJRaqWpmKfE4THEDUz7IIPSH8REMhZq4IBfv34dw+EQ165dQ7/fD+VsvJ8HbdV1YHOhtM1UO6qKrFFASpNQyjjLjr2vmAhIepiVONB21oQ6HZkqYletprWlTAXMMxOZmJbNZhOLvRwfH6NSqaBer+O5557Dl7/85RD8JcgHEJanApLecrZRvy2tWuolTd2pRNLvtOPWpZ1XZ/qGkYmoHnSfzrUH5h5pDXPQnFcpwOO48K9ei3E8Wl/ESbw+LR9gOm2IBbhiDkxrgekzpe3juXqt2H6LhfTb3ue+wEQ6sDEiNomBVR0Iqi6LCzSRjOfwd7VaTcwqYaBV5+ITfzFEwk+r1QpMHpuBoma/JV5jWThDn8X+t8yRJnE2VWc7x0QWQ9gOIRPF3lyN0BNbqUVFBtVrcJ69Zg84N1+rjGA6k5nOI+N8ek6V1rU+rDWob7+VqEqUjDEH4zIVxePZRn74TEyXScvnXpV2iokIEHVmhwWNaXE16721xG0E0gx1sKM12EpG1LwbFsWq1+tBfVDVcaBsxN62hWrL+nj4vSlmiZEy8DaqDNhhJiJpdiH3x8S/mu78r7NjGXQFkAiNUEpRhWolWJUsZBJWA0mTEJSG+uarmR9jMs2aTAvZxPpqnX4F1ltPVmmnmAhIAmFNJAPmb6uVRBqu0DecINy++ZoFyfRVMq+mlFCdcao1PdYAQvaibRsHifdYxQsdexbSWUinTSUQaaeYSL3PZBQFtWSAmEkcE9tqbank0GxGep8paTTVlu1QIK9qj9dTpkujNEmzKpMs8x2teo37wsTXop3WuwvMk99VOinTWIcaU1mVdI4ZgBA0Jbgm1uH1NFEfQEgL0UHVWbAk3a/z1KyaPEumOo3um7CH1dmq2lRlWAaz35Q+zANS6UVJwmPVQiOjqnShtKEU0zXOtIq/tpnnaXhGn1EtpmVgnJRm3tvjlwHpTfAQsGNMpKDYppAq2FV8w3M0c5HnTybTAqEq3fjRQWcdIZ5L4K1Tk+gbYpKa3l+LJShG0m/LKMqkNhC7jGFsP6X14yoOyFVpp5hIPcD2TVNrJ83Ep8TRt5/Tn3kMr6UrVE8mk1D1lZKGeIwDTSuOedWaHWktSPvhPTY15+2zqqRb57z7QhIBSEgMUpqIpoSKTbWhn2cymYTMRZuTrdKh1+slovBUd3bwyZRaREItQcvEMTeEkrXENgHJq1pzmy7Ct1NMFHtrFevEwDaJ0kmTz1Qa6NRrVUM8lhYapY2qKLUUs9ksyuVyonIJkMw+sN7nNBeFJo/FnknJgu+0Y5dd475QZ8QaQHqmHxkBmDvPKAE0BVUZEoinmKiEGQwGIQrvvQ+eaE6FHo/HIcDKxDe2hVjJtjOt/cumNW1Ly/DSfeOxTqNlHUDGsTEi/a2VQ4hfVKoxiEr3gSbvK9ZSlQYg4CYb2AWQuE8s30fV4WnPvSrYjv3nc99X6mzVfRwgBd7WEWk92NymDMVt9BdR2ujSD4pXaKE5lywAYX1Wq3is+RwqQVbxcqdtixkc+r0J7RQTMb8ZSO8MUiy7cVl9HwZLOT+MzEbpMplMwkpBPEYT2KyfSSUVr6+MppjHJqnxebTghM1uXEax/XbbpqorRjvFRPqmpxEHVc3qGMiOTdPRlRkp2hWf9Pv9MM8eQFgchtfUKiSVSiUxiZKZAbatvIfdphTDVMtoFfAcY6pNcomAHWMivr02hEC1YN9UOzj60fLDPIcDzeR+Pde5aXK+rsVhnZSUdN77BabRAqRAUlKqhLKqS2N1m9Cq5j3vtwntDBM55xLlYSyDWOcd96uTz75tFmBTYsSO4zYWpAKQ8AeRcZSB6JSkS8Cu+kjGSqtjrZgpTRKt65SMMco2eAjYISYiWSxBIgNxUIA5AymlYScyBnGMRvK1vAyriulgaJakYiCmhdAPRXDNdmq9gBgTxXxFdv+22MaGgTahbQt//pRz7gvOuc87537DOVdyzj3inPuUmxb4/LCbVgyBc644+//0bP/D697PmukqeTS8YNfYALBgpfF6MTXIWa1aKY2D2el0Qj0frXnE/eqHIlDXKdy8j1UzMfymgxpTadbbndZnaaR9tw1DblOz8cUA/hWAx7z3fxdAFsCbAPw8gPd6778ZwG0Aj89OeRzA7dn2986OW5kUXPKBrbVDDMGgKgd41t7pA4tFRUmj4Qj+1oQynu/9dO4Yl5vSeWhsg4Y1CLzplLSrQVqfkWUSlYCx/esOumVeu29T2lad5QCUnXM5TCvHPgfgtZhWTQMWC39+cPb7owBe59ZouZ1qQ6kCJEWyzjDVpTN1Dn0sNEIJxXiaMiSQLHXH6dDEP2neZU5uVEkUcyryt/U1xfanURqATjvHSp5t1OI2ldK+BuA9AL6CKfM0MC3+eey9p+uTxT0BKfw5298AcMle1zn3VufcZ5xzn4ncc+HB9a1mUJXpqmQCprTGin/G8pOkLaFOo2YQ0F/kvV8oukUJSSnCUA1jbVqqGFg+kdHiLluAIa3dq/y2pv6mCWnAdursEFPp8giAFwGoAvieTa9H8imFP2OYQCWSWlA00clMlEr8qOPPmtZaTErVnt6j2+2Gcn7qJyLj6jVosWlqLUmlkk6tnvWv7e+VY2mrCvhtrTLSNursHwH4svf+uvd+COC3Abwa0/rUdJBocc9Q+HO2fx/AysVw7JquacS3SaWSFryiqc1YGeNaNNXVf6Qea5Uyw+EQ3W53oSQfmU2tO87+AJAIzKp7YdYnCxhPGYdMuOyZY2SNjztB2zDRVwC8yjlXmWEbFv78JIA3zo55C5KFP98y+/1GAH/g15Cfy3Q791srS7MO9UPGUFXHa+l0Hutr4rUnk0liKXJNkyWusnhpPJ4v2WAH1RoJuk8lZZrT8TTAvUo3r1u7WmkbTPQpTAHynwH43OxaH8B0CYafds49jSnm+eXZKb8M4NJs+0/D1Lw+jYhJYpYNyXamms3KUJz9qZhH1eKyDERSs9kMzMgVqTXOxkAtLThKLU1nYRvJaJrDzX2nWXAWF8b6YdmxaX23Dm1b+PPnAPyc2fwMgG+PHNsD8AOb3os1gEgqeSxZ35EeT4YgY+kMklk7F3xJVkIQF3FZg1KplJBMAMK1Wd+o3W4nLMWY51yZhD4nvXfM5RB77mX9Edu2rZrbKY91LFBprRjLODFMQGnjvU9MMmQkX69rLScyG4tZMZYHzBcoZlU1eqM1HMLofmxALfC26itWEjlG61pZ3m+eSwTsEBNZ09zSaWLfnksmGQ6HITrvvUetVgtYRo+3gdHRaBTiaDTd1VGpnutisRgYgjgsFu5ICyzrM5yWwbApvSB+ortNauVYEbyJflem1DJ5k8kExWIxSJ1YXIsMwxkgwGJRczIacRDX+tAYGkkZ1DKwVWf0FZ2FpXVWVtvOMJGN3CtZqUEG0Q8pZs1xP10CCnbVF6XAGUBY4AWYx8VUfVl/Ea+pZZTtC5D2jDxu07SQZRZczOm6Du0ME9Ga0oe1fhWSMoFVbWoFcbudccr9ZBreV+/lvQ8le3mOFq9SByVzrjmfTc18S3od+rX0eWwKb4xi20/DUJsmpAE7xESac2PfIvtf31b19ZAhNLDJc2ygVpfD4vkKtun70WIPalFp2RoOvnMuzNXXNlhvtX1Oi5PWZZIYWYNjG5W2M0yURjHraRlzWG8xgbCmlwDpakXDH8RFdt0N3pNYS+eoAVjILdK5bsAcF8UGNi38swwj2n6yx9gXcV3aGSZKM+kt0T9jHYYkqqbxeBwchBp8JFZSj7Wea511nU4nUUpPMZAeS/OdwdxY/E5xjwZcY+o4rX9WJZVC7INNaSeYiD4ctc7SiExElcNBUiykoQm1pvhJS1fVAaUkYQyN7VRnpvpfVEIWi8WQOqu4SxlEJZQytDUUztJK25R2golizkJSLE6kQNQOTuy6+js2L01VnfXrMHnfVkWj74hSjrWQ6NCk01FVI0lBdaydZxHN30Z9WdoJJiLFRO5pup7HWMxgwxsqDez1Y/hEMyFtESy2k0xEh6QytNY4slJGrTCSZbJlfbCqn8w+y6a0E0wUc/it0lFqrivoXWb1LJN2mqdNhhiNRuh0OsFKYzqs5idpMNa5acCWDkh7fz6vBdfWmlvWD6tKmbNQhcAOMVGaxLGAW/GIvl2KZ1aJO1lm0XaoGgIQgq8K6mPYhemxTIyLMRGlWWyyI9u1zOEY894vY5azYKSdYCJSmqpS0vQJMpECZZt4ZqWMPTfmJ1JPdyaTCeX4FDyPx+OQmM8q/DoRkRjJuhQ46Fo7UplG1W4MD27iaNwm+ArsCBOtOoWYb7AOph0kO3UoLe4WY1INqur20WiUWFGRjkqmgtAS5H9KIutAtc7JNINA42u2jav4iCxtC7LvKSYCkPDPWOIbZ31CNsleTX91ALLQA5BcaA9AANfWLNfrqXuAKbpWZSk2S5tCbZ2Xm6qkNKNhXdoJJkpzMOq2NFAMzHOfASR8OjxPr2G32fLAmimp6qPX6wVG0vllCrCB5CwPSiMyjEolqizrUrDmvzL/OqR9tY2jEdgRJtLMQ9IyhomRLTMTkzYkO71ZJRTJhkj6/X6ork8G0MkBtNCo0oiJmC6rGEhJg7pKsVztdUif5b5gIi2iCaSb4zFfzzKLzMba+K2L7llJZR2PaupzFZ/RaJQw54Fk1X6qo2KxiGq1mgDj+ixqFWp7ASQ8+JuQBerb0E4wETAPGyx7eCstSLZiq34DSa+0xtKYcM83VTMYY/fgonm66qICek4lIhMyhkbcE3s+NfX1eRUXpVmqq9J9AawVU8RSJpSY46yg1vqNrATS+3CfmtdUKZp4Fut4BnR1TTZKDK26YUMgzHpUUsaIgWtiqm3pvjHxbTpp2htH/wrJlsyzXmHFPQq0Y7Eza5VpG4h11OmorgDrL6L0USvNSiAF2mkqxxaIYHvvNu0EEwFJ8GfFvgWJsTdXZ7La7bpNl+fU/RZvqcXIfcRFBNeUYFRtmrDGwqD5fB7lcjnB5FbCUhpaJ6MtocPzL8IeKaSDph5jKxGsmoo5B2NgW7EL9ynQVVyk11YaDAbo9XqJtT4YfGXbaCQQWDOir0Cd7VDAbZlFGWkT0j6459UZ31bFMaqSVOSnmeN8+y0m4m/90BRX5tNr8e23KoTX7/V68H46n02TyrifwVma+sRaXAWbz6WzYXnP2Dy0TZnoLOncMxFJZ1soYFbLhqY2t3EA9Vwgmdes23U2h2Ibxtp02jSQdPSR8VhJjUxENcRga7/fD1iMzkYCbGsI8CXh9cmU+uJYU3+ZS+NO0U4wkQ16kmxuMikWhAWSWEqTz3ismvP2eB1cmzmp1yCm0hmwzrkAnqnKgHkRLKaG2Lbr82o8TV+kWN2idbCOxYSb0LlnIp0rb937Kv75neZVJsWAJ5lDC3oqgOZ5/K9Sgfv44dpolGaq0giu+UyURMy7Vn+QfnTdWZuWwutoW/W57LOrNNZn34ZOZSLn3K845553zn1eth05555wzn1p9n042+6cc7/kpsU9P+uce4Wc85bZ8V9yzr1l1QaqqgEWg7EMF8RyjmzEXq9pO0/9O1RdBNc2mZ8SQP9zcDmTlkypzkReWxmJU4hKpVJgBjKpuhXU2tM2Aovea8soJPvMaa6DdWkVSfRfsVgB7R0APuG9fxTAJzAvE/O9AB6dfd4K4P2zxh5hWj3kOzCtGPJzZLxVSH01sQ6yADvmWORvku08jaPFzGzu03BIbADG43GY29/tdhfwi628T0xkMx1VffE/kLTGlLliXu3TJEzsBduETmUi7/3/AnDLbH4D5kU8P4hkcc9f81P6E0yrpj0E4B8DeMJ7f8t7fxvAE1ixNB+BqeIRG8G2b26sU2L+IRLVjHq21cGoeCh2LVVr3s9ngLBIBEMdamVyOwE+pZFeU6UrXxQ9h5TJZFJTbU+jbYOvwOaY6AHv/XOz318H8MDsdyjuOSMW/kzbvkDOFP5U3Z0GpPmWE8iSIWh9xYC5+oH4W++n145lFqoFqMcCcwuMHzI9JYvODCEWIsiOLYCj0jGWeqtS6zRcZJ/tLGhrYO2nrTwze9KnFP60g61vKdWCLQqVFom310xTeYp7LOZQ1aaSkWkhrHuks0A42ATw3vsQO7NMREZRP5FeQ/uA2+w8/TRzX627s6BNmejaTE1h9v38bHso7jkjFv5M234qWQ+xWmRqvQBzk1/1vPpybMcqM8Q83TzG3lcTybhf1Q7BNYCFadS5XC5R1ZZ4iOEPW+uabeG1gcVKswAS/XAaKfNsU6uRtCkT/S7mRTzfgmRxzx+ZWWmvAtCYqb3fB/DdzrnDGaD+7tm20xuYWSzvYvcXi8WFTMNYqgd/x1wFPMaC6th16NXWj77Zk8kkLGnV7/cBzFUZv7UIhOYXERfZdlrJZBmeAF1VWpq6Ois1RlrFxP8NAP8HwLc45551zj0O4N0Avss59yVMSxG/e3b4xzGt2fg0gP8M4F8AgPf+FoB/B+DTs8+/nW1biazvRzuJoFIDp2py8xhVfTrgqiL1+rGOVl+V5iWpGuV2Lt1AiaRTiQAkGJG4qFgsolwuJ6SsPoNuj2U8xqYZLWOYszLxTw28eO/fnLLrdZFjPYC3pVznVwD8ylqtQxJvAEioD8UCFMuxQKrmGMWAp50WZAeP5zN4qqY1VZNlXOZc12q1hF9ILUlenxKEqo24icT2cDszJzmLRPfbF2oZee9fUHV210jBIgdZVQwLI6gz0IJk7rdpHRxUO41YMZWqOAtYLR7St5qpsqr+lJHoL6IaIpMyDGI95iS2xcYMSarSFAemgeizANfnnolUfQGLTsZisbiQE22trmUORCDJdDp4Kv0sA3K7DpBuJx6iv0jboE5HYiMC6lKphHK5vGBAxJ4/5mAkU/IZYudp289CnZ17JlJgrcROLhaLAXcoXrE4imSlhoJ1O/9MAbcea0179UDrOSzFR5VBFWhneNAyIzOxkpqqR0pO5mWrBLP9Za20NEZRh+o2dO6ZiN7eGNglKGWsimRVnpUqPFeliC2MRVIXQOx6LBGjRFXDhWRs/SKtHmLvXygUUKlUoklolqmsR1v7JIb/9Hqk2KSDdencM1GsI2x6BM1obkvDEXpNDgoHS0MclmmUsSzz2XuQ6Zxz6Pf7iWlEykSci8ZzNemMuEjbrfdUJ+OqM2TP2qxXOvdMBCziIrWOgHllWSAeoVfgrBJJz6F32kbr9be9vi69oI5JYDqQxEP0XNs0WeIi731ITKO606U+VQrZUjlacobXVpW2jHliMGETOvdMtCz9k8USbCJZbPDTnJV6TJqYt5iKTMd9NL2VMb2fpoK0Wi0ACOCazKpq2pr3pVIJlUolPItOvWYbKLlsSISkU5WW+b3uG3UWG2SK/Xa7vSClmM7K8wkgNRjL32mZgQq8dZ/mGvFYjWfpNQAkpldTSlANeT/PxQYQ1q4lE1nLTO+nvxU/UVIRfMfalPZ/U9opJiKxE8vlMlqt1oJqUmsqJq6VAfgWx+JtlBKKh6xa1HJ6sTazyAPTQjSHiHhOS9JQ5VnPtXqnFX9ZrzWfW9W9pbPGR+eaidTJZyUFzeFOpxM9PnYd23lqYuvbrMdaJ6RzLrEADJBM4eX9eJ1+vx9qF/V6vYSTk55rOiOdm0+ttrhIn4PnKja0DMbnSvMxAUikA29D55qJgOWg2jm34OK3prP6beyHpKtV8xxeS62xNGYm5lCJwPPIPOq55swOqjRabjrzo1KpLCywp34sjaVpQNaqWftS3Qkr7dwzkeIYfYMVVFuxDyS9z1qgwV7bOh9VPSiWUhWpUistXZb3n0wm6Ha7C9OrVdrRSlMfDz3X6s7Qc+w17HYgPkM29ntbOtdMFHu7SJVKBd1uN7HdqiBlwNgHSM4uJSMoVophLX2zdTYHME+y13OZLqtMxBeB6pEWG1VTqVRCvV6PDraNqxFnxZyemjarz0gJeBZ0rpkIWHQ2EixWKhW0220Ai85Am9rBb+vj0WQ0a97HPMRAfOlxHVRNDeE9e71ewEYcOFv4U+fac3u1Wg2MpupJg7lsTwzYkzFjvrNY325KO8FEFnM458K6qjGzVdWPSifFFNqpihsUS2kuNM9NY1AyECWTtpuFQTXgyudh3Wu9LiUS5+rzOaz6Um91TBIpiNdt+jkLOtdMxDeIpG9cLpcL1o5VZ5Y5LD7SztNwh4Ye+FFcRKJjUa+t97TYYzweo9PpBFNfVSAj+iykrpMZy+VyiKMpfuJ1LSPoS8ZvzaOymO0sgq/ADjCRlSialK+zP7RDY2Lagmq12KzPJy0coNePgXS2T3EPz6GFpuuj0R/kvQ8ebb1XsVhEpVJZ8Fvxfmql2XLG/CajxvrlvmAiYNFhyM7lYOjAKt7R82OAWCWJZSY9BpiDZWulxbCVnqdvf6/XC1F9PUaT1BhfY/ErWmixBDT+VulpMxEsLop9zoLOPRNZvwcwzSE6OTlJ7FMsE3vDrK+I2zh4MenC6+g19RrqOrD3VWmQycyr7lOCajHTfD4fwiLqxshkMqhUKkGFW8CvxgKQjovSVP5ZxM2AHWMiPnSpVEKj0VhQX1aqxL51wBXQkpmsT4lvsuIeBeuqCoFkTrh+OI2IVppKokqlEoA8zXIug67Tq2NGhEo7SiP+5n7FRXr8faPOVGxTYuTz+TDP3Wb2KdPo/9g0afvbYiF2NpmNx9m1QawUYoReTX3vfVgXjVKHx1Hd6KRGMq71XKepXPtf22aZi9/3hToj/uGbyUHhgDHqTWDJWJQ67pxzC5jABi1VwlgPNUkxkAW3lCJWNSqmymQyYalPMhHTNTjA3W43vCR8pkKhgFqtlmin3pftjj2PklVzacdtQi98rbYlpG8QQwi5XA7dbjc45OhxJsbg4DHBS/0oOvdLHXXAXA1xO3/TEozl9FB6aHVba2FpAhqLgjKiTxXDtlGi6fx8TZe1TMT7a1/ZTE1gsSAF95+VOjvXTDQcDnHt2rXQye12O6wvb9UQpdK1a9fQ6XTC28m3dTQaBbWgq0fH/CUcYCvuYzE6ZSr1DpNBlCEZhGVOeD6fR6/XS7gChsNhyNvmAjKVSiUUrCDZtineUVymroc7YZkB55yJ6D/RHOrT6NlnnwWwKOIZ3OQ+AIlcZgBhBqqazAcHBwkAW6lUghSkD6ZSqSRMd60NQKKFxUmN3W43zMGn5GQCG6Uqmaher6NcLifmqmm8jYCcEtPeP4ab7hsm2oQsoFayaSPNZnPt69swAoOlqs5qtVpikEqlUljDo1Qq4fDwEN/6rd+Kw8PDANIZbC0Wizg6OkI+n8fe3h6Oj49xcnISZr5a7zlfFut45T5gLnmpLnn8WWEid1YXuhPknDu/jTtjspF/OhvVJcF61xwzLu2gQL5YLAYVTZVJNUosxjSa559/flmTFsh7HxVf552JmgC++EK3w9BlADde6EYYuhtt+jve+yuxHeddnX3Rm2JXLzQ55z5z0aYknWs/0QXtBl0w0QVtTeediT7wQjcgQhdtMnSugfUF7Qadd0l0QTtAF0x0QVvTuWUi59z3OOe+6KbrhLzj9DPO7L4vdc590jn3pHPuC865t8+2r72eyRm3K+uc+3Pn3Mdm/x9xzn1qdt8PO+cKs+3F2f+nZ/sfvhPtUTqXTOScywL4j5iuFfIyAG92zr3sLt1+BOBnvPcvA/AqAG+b3Xut9UzuAL0dwFPy/+cBvNd7/80AbgN4fLb9cQC3Z9vfOzvuzpLNwDsPHwDfCeD35f87AbzzBWrL7wD4Lkw95w/Ntj2EqSMUAP4TgDfL8eG4M2zDSzBl3NcC+BgAh6mHOmf7C9P64N85+52bHefuZB+dS0mENdYCuZM0UwUvB/AprL+eyVnSLwL4WQCMvF4CcOy95xRWvWdoz2x/Y3b8HaPzykQvODnnagB+C8BPeu9PdJ+fvuZ3xTfinPs+AM977//0btxvEzqvsbON1wI5C3LO5TFloA957397tvmac+4h7/1zbrX1TM6KXg3g+51zrwdQArAH4H2YLgOWm0kbvSfb86xzLgdgH8DNM2zPAp1XSfRpAI/OLJACgDdhum7IHSc3zcX4ZQBPee9/QXatu57JmZD3/p3e+5d47x/GtB/+wHv/QwA+CeCNKe1hO984O/7OSs0XCjyvACZfD+CvAPw1gH9zF+/7DzBVVZ8F8Bezz+sxxRWfAPAlAP8TwNHseIepJfnXAD4H4LE72LbXAPjY7Pc3Avi/mK6j8t8BFGfbS7P/T8/2f+Od7rOLsMcFbU3nVZ1d0A7RBRNd0NZ0wUQXtDVdMNEFbU0XTHRBW9MFE13Q1nTBRBe0Nf1/rhqQi/nb3JwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Prediction : \", predictions[0]['labels'])\n",
    "plot_image_from_output(images[0][0],predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\spiro\\OneDrive\\Υπολογιστής\\Thesis\\Project\\master_thesis_dtu\\notebooks\\model_test.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X66sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# load the pickled object\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X66sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../losses.pickle\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/spiro/OneDrive/%CE%A5%CF%80%CE%BF%CE%BB%CE%BF%CE%B3%CE%B9%CF%83%CF%84%CE%AE%CF%82/Thesis/Project/master_thesis_dtu/notebooks/model_test.ipynb#X66sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m loaded_object \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(filename, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\serialization.py:795\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    794\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 795\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1002\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    996\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(f, \u001b[39m'\u001b[39m\u001b[39mreadinto\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m (\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mversion_info \u001b[39m<\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[0;32m    997\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    998\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    999\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived object of type \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(f)\u001b[39m}\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1000\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfunctionality.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1002\u001b[0m magic_number \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39;49mload(f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[0;32m   1003\u001b[0m \u001b[39mif\u001b[39;00m magic_number \u001b[39m!=\u001b[39m MAGIC_NUMBER:\n\u001b[0;32m   1004\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid magic number; corrupt file?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\storage.py:240\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[1;34m(b)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_bytes\u001b[39m(b):\n\u001b[1;32m--> 240\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(io\u001b[39m.\u001b[39;49mBytesIO(b))\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\serialization.py:795\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    794\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 795\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1012\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1010\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1011\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1012\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1014\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1016\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\serialization.py:958\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    954\u001b[0m     obj\u001b[39m.\u001b[39m_torch_load_uninitialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    955\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m    956\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m    957\u001b[0m     deserialized_objects[root_key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m--> 958\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(obj, location),\n\u001b[0;32m    959\u001b[0m         dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    961\u001b[0m typed_storage \u001b[39m=\u001b[39m deserialized_objects[root_key]\n\u001b[0;32m    962\u001b[0m \u001b[39mif\u001b[39;00m view_metadata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\serialization.py:215\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 215\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[0;32m    216\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[0;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[1;32mc:\\Users\\spiro\\anaconda3\\lib\\site-packages\\torch\\serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# load the pickled object\n",
    "filename = \"../losses.pickle\"\n",
    "loaded_object = torch.load(filename, map_location=torch.device('cpu'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot validation loss over training\n",
    "plt.plot(list(range(0,epochs)), train_loss, color='blue', label='Train loss')\n",
    "plt.plot(list(range(0,epochs)), val_loss, color = 'red', label='Validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.StepLR at 0x18b0c2fa5e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "0b00de58b91327812a3543b0a54b44486ebb363ab132784d28226d0402cd7527"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
